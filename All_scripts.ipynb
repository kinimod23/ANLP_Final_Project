{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Political Stance Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier kommt nochn schöner Text hin oder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](images/pipeline_ANLP_project.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd4a985b561a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# All imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTDOUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import re, string, gensim, os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from tokenize_articles import read_corpus\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_headlines(cleaned_article, output_file_hl):\n",
    "    '''Extracts just the headline from the raw data and saves it to a text file. \n",
    "    Used in conjunction with the function clean_corpora.'''\n",
    "    \n",
    "    with open(output_file_hl, \"a\") as output_hl:\n",
    "        hl = re.sub(r\"(BYLINE:.*|SECTION:.*|BODY: .*)$\", \"\", cleaned_article)\n",
    "        print(hl.strip(), file=output_hl)\n",
    "        \n",
    "def clean_corpora(input_file, output_file, output_file_headlines=None, guardian=False, sun=False, telegraph=False, guardian_hl=False, \n",
    "                  sun_hl=False, telegraph_hl=False):\n",
    "    '''Takes the raw LexisNexis files as input and extracts the headlines & articles and just headlines (optional), \n",
    "    writes them to a text file. Due to subtle formatting differences between the newspapers, \n",
    "    there are slight differences in the preprocessing'''\n",
    "    \n",
    "    text = open(input_file).read()\n",
    "    split_text = text.strip().split(\"All Rights Reserved\")\n",
    "    \n",
    "    with open(output_file, \"a\") as output:\n",
    "        for un_article in split_text:\n",
    "            un_article1 = un_article.replace(\"\\n\", \" \")\n",
    "            article3 = un_article1.replace(\"\\\\\", \"\")\n",
    "            article2 = re.sub(r\"(BYLINE: .* [0-9]* words|SECTION: .* LETTER|SECTION: .* words)\", \"\", article3)\n",
    "            article = re.sub(r\"(LOAD-DATE:.*|LOAD-DATE)$\", \"\", article2)\n",
    "    \n",
    "            if guardian:          \n",
    "                article_g = re.sub(r\"^(.*GMT)\", \"\", article)\n",
    "                print(article_g.strip(), file=output)\n",
    "                \n",
    "                if guardian_hl:\n",
    "                    article_g1 = re.sub(r\"^(.*GMT)\", \"\", article3)\n",
    "                    extract_headlines(article_g1, output_file_headlines)\n",
    "                               \n",
    "            if sun:\n",
    "                article_s = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;)\", \"\", article)\n",
    "                article_s = re.sub(r\"([a-zA-Z.]*@the[-]*sun\\.co\\.uk.*)$\", \"\", article_s)\n",
    "                print(article_s.strip(), file=output)\n",
    "            \n",
    "                if sun_hl:\n",
    "                    article_s1 = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;)\", \"\", article3)\n",
    "                    extract_headlines(article_s1, output_file_headlines)\n",
    "                    \n",
    "            if telegraph:\n",
    "                article_t1 = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;|Scotland)\", \"\", article)\n",
    "                article_t = re.sub(r\"(Copyright [0-9]+ Telegraph Media Group Limited|BYLINE: .* BODY:|[0-9]* of [0-9]* DOCUMENTS|HEADLINE:)\", \"\", article_t1)\n",
    "                \n",
    "                if telegraph_hl:\n",
    "                    article_t1 = re.sub(r\"(HEADLINE:|Scotland)\", \"\", article_t1)\n",
    "                    extract_headlines(article_t1, output_file_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(input_file):\n",
    "\tcorpus = []\n",
    "\twith open(input_file) as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != '\\n':\n",
    "\t\t\t\tcorpus.append(line.strip())\n",
    "\treturn corpus\n",
    "\n",
    "def tokenize_corpus(articles):\n",
    "\ttokenized_articles = []\n",
    "\tfor sentence in articles:\n",
    "\t\ttokenized_articles.append(word_tokenize(sentence))\n",
    "\treturn tokenized_articles\n",
    "\n",
    "def write_to_file(tokenized_articles, output_file):\n",
    "\twith open(output_file, 'w') as o:\n",
    "\t\tfor article in tokenized_articles:\n",
    "\t\t\to.write(\" \".join(article))\n",
    "\t\t\to.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(input_file):\n",
    "\tsentences = []\n",
    "\twith open(input_file) as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\tsentences.append(line.strip())\n",
    "\treturn sentences\n",
    "\n",
    "def annotate_articles(sentences):\n",
    "\tannotated_articles = []\n",
    "\tfor sentence in sentences:\n",
    "\t\tsentence.lower()\n",
    "\t\tp = Popen(['/Applications/Treetagger/cmd/tree-tagger-english'], stdout=PIPE, stdin=PIPE, stderr=PIPE, encoding=\"utf8\")\n",
    "\t\tout = p.communicate(input=sentence)[0]\n",
    "\t\tarticle = []\n",
    "\t\tannotated_words = out.split(\"\\n\")\n",
    "\t\tfor word_anno in annotated_words:\n",
    "\t\t\tword_anno = word_anno.split(\"\\t\")\n",
    "\t\t\tif len(word_anno) == 3:\n",
    "\t\t\t\tannotation = (word_anno[0], (word_anno[1], word_anno[2]))\n",
    "\t\t\t\tarticle.append(annotation)\n",
    "\t\tannotated_articles.append(article)\n",
    "\treturn annotated_articles\n",
    "\n",
    "def get_lemma(articles):\n",
    "\tlemma_articles = []\n",
    "\tfor article in articles:\n",
    "\t\tlemma_article = []\n",
    "\t\tfor word, anno in article:\n",
    "\t\t\tif anno[1] == \"<unknown>\":\n",
    "\t\t\t\tlemma_article.append(word)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlemma_article.append(anno[1])\n",
    "\t\tlemma_articles.append(lemma_article)\n",
    "\treturn lemma_articles\n",
    "\n",
    "def write_to_file(lemma_articles, out_file):\n",
    "\twith open(out_file, \"w\") as out:\n",
    "\t\tfor article in lemma_articles:\n",
    "\t\t\tout.write(\" \".join(article))\n",
    "\t\t\tout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(processed_file, output_file):\n",
    "    '''Takes the lemmatized, tokenized file as input and removes punctuation, stopwords and other strangely formatted\n",
    "    words/punctuation'''\n",
    "    \n",
    "    stop_words_nltk = list(stopwords.words('english'))\n",
    "    to_delete = [\"brexit\", \"``\", \"''\", \"'s\", \"·\", \"wo\", \"n't\", \"...\", \"@card@\"]\n",
    "    \n",
    "    stop_words = stop_words_nltk + to_delete + list(string.punctuation)\n",
    "\n",
    "    with open(output_file, \"w\") as output:\n",
    "        to_be_filtered = open(processed_file).readlines()\n",
    "        corpus_list = [line.strip() for line in to_be_filtered]\n",
    "        corpus_nlist = [each_word.split() for each_word in corpus_list]\n",
    "        for sentence in range(0, len(corpus_nlist)):\n",
    "            for word in corpus_nlist[sentence]:\n",
    "                word = re.sub(r\"^(\\')\", \"\", word) #getting rid of the ' at the beginning of some words\n",
    "                if word.lower() not in stop_words:\n",
    "                    output.write(word)\n",
    "                    output.write(\" \")\n",
    "            output.write(\"\\n\") #line break after each article/hl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-312a8e022181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "class LDA():\n",
    "    '''\n",
    "    This class implements a Latent Dirichlet Allocation. Initializing one instance creates a\n",
    "    model and saves it to a folder. It also uses this model to create a topic distribution\n",
    "    vector for every article in the corpora\n",
    "    '''\n",
    "    def __init__(self, filename1, filename2, stopwords, num_topics=10, no_below=20, no_above=0.5):\n",
    "        self.num_topics = num_topics\n",
    "        self.corpus = self.createCorpus(filename1, filename2, stopwords)\n",
    "        self.model_corpus, self.dictionary = self.createModelCorpus(no_below, no_above)\n",
    "        try:\n",
    "            self.model = gensim.models.LdaModel.load(\"LDA_Models/ldamodel_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above))\n",
    "        except:\n",
    "            self.model = self.train_lda()\n",
    "            self.model.save(\"LDA_Models/ldamodel_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above))\n",
    "\n",
    "        self.corpus_feature_vectors = self.apply_lda()\n",
    "        self.final_output = self.createFinalOutput()\n",
    "\n",
    "    def createCorpus(self, filename1, filename2, stopwords):\n",
    "        # Creating lists of tokens in a list of articles for further processing\n",
    "        with open(filename1,encoding=\"utf-8\") as f1:\n",
    "            with open(filename2, encoding=\"utf-8\") as f2:\n",
    "                all_articles = f1.readlines()\n",
    "                all_articles.extend(f2.readlines())\n",
    "                corpus = [[token for token in article.strip().split(\" \") if token not in stopwords] for article in all_articles]\n",
    "        return corpus\n",
    "\n",
    "    def createModelCorpus(self, no_below, no_above):\n",
    "        # Creating a model corpus and a dictionary for the lda model\n",
    "        dictionary = corpora.Dictionary(self.corpus)\n",
    "        dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "        model_corpus = [dictionary.doc2bow(token) for token in self.corpus]\n",
    "        return model_corpus, dictionary\n",
    "\n",
    "    def train_lda(self):\n",
    "        # Creating a LDA model from the whole corpus\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(self.model_corpus, num_topics=self.num_topics, id2word=self.dictionary, passes=10)\n",
    "        return ldamodel\n",
    "\n",
    "    def apply_lda(self):\n",
    "        # applying the LDA model to all articles to get the feature vectors\n",
    "        corpus_feature_vectors = []\n",
    "        for article in self.model_corpus:\n",
    "            corpus_feature_vectors.append(self.model[article])\n",
    "        return corpus_feature_vectors\n",
    "\n",
    "    def createFinalOutput(self):\n",
    "        # arrange the feature vectors in a list\n",
    "        output = []\n",
    "        for vec in self.corpus_feature_vectors:\n",
    "            vec_dic = dict(vec)\n",
    "            output.append([(vec_dic[topic] if topic in vec_dic else 0) for topic in range(10)])\n",
    "        return output\n",
    "\n",
    "    def get_topics(self):\n",
    "        # show the topics\n",
    "        return(self.model.show_topics(num_topics=7,num_words=3))\n",
    "\n",
    "stopword = stopwords.words(\"english\") + list(string.punctuation)\n",
    "\n",
    "filename1 = \"Corpora/filtered/filteredjust_hl_article_guardian_tokenized.txt\"\n",
    "filename2 = \"Corpora/filtered/filteredjust_hl_article_tokenized.txt\"\n",
    "\n",
    "\n",
    "# no_below : No words which appear in less than X articles\n",
    "# no_above : No words which appear in more than X % of the articles\n",
    "if __name__ == '__main__':\n",
    "    for num_topics in range(3,8):\n",
    "        for no_above in [0.3,0.4,0.5,0.6,0.7,0.8]:\n",
    "            lda = LDA(filename1, filename2 ,stopword, num_topics=num_topics, no_below=20, no_above=no_above)\n",
    "\n",
    "            corpus_feature_vectors = lda.corpus_feature_vectors\n",
    "            output = lda.final_output\n",
    "\n",
    "            #pprint(output[:2])\n",
    "\n",
    "            k_means = kmeans(output,str(num_topics)+\"_\"+str(no_above))\n",
    "            k_means.plot_elbow(range(2, 10), \"Elbow_plot\", individ_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3281267fe026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "class kmeans():\n",
    "\n",
    "    def __init__(self,feature_vector,image_dir,r_state=42):\n",
    "\n",
    "        self.feature_vector = feature_vector\n",
    "        self.r_state = r_state\n",
    "        self.dir = \"cluster_images/\"+image_dir+\"/\"\n",
    "        guardian = np.ones(998)\n",
    "        sun = np.zeros(876)\n",
    "        self.both = np.concatenate([guardian, sun])\n",
    "\n",
    "\n",
    "    def cluster(self,k):\n",
    "        clustering = KMeans(n_clusters=k,random_state=self.r_state)\n",
    "        labels = clustering.fit_predict(self.feature_vector)\n",
    "        self.labels = labels\n",
    "        self.centroids = clustering.cluster_centers_\n",
    "        return(labels,clustering.inertia_)\n",
    "\n",
    "    def visualize_data(self,fname):\n",
    "\n",
    "        X_reduced = PCA(n_components=2).fit_transform(self.feature_vector)\n",
    "        fig = plt.figure()\n",
    "\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=self.labels,marker=\"^\", s=both*10, edgecolor=\"red\", linewidth=0.3)\n",
    "        ax.scatter(X_reduced[:,0], X_reduced[:,1], c=self.labels,marker=\"o\", s=((both-1)*-1)*10, edgecolor=\"black\", linewidth=0.3)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        #plt.colorbar(scatter)\n",
    "\n",
    "        if not os.path.exists(self.dir):\n",
    "            os.makedirs(self.dir)\n",
    "        plt.savefig(self.dir+fname,dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_elbow(self,k_range,fname,individ_plot=False):\n",
    "        distorsions = []\n",
    "        for k in k_range:\n",
    "            labels, score = self.cluster(k)\n",
    "            distorsions.append(score)\n",
    "            if individ_plot:\n",
    "                self.visualize_data(\"cluster_\"+str(k))\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.xlabel(\"number of clusters: k\")\n",
    "        plt.ylabel(\"Sum of squared distances of samples to their closest cluster center.\")\n",
    "        plt.plot(k_range, distorsions)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(self.dir+fname,dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_histogram(self, fname, k):\n",
    "        #does not work\n",
    "        grouped = [[] for x in range(k)]\n",
    "        # the histogram of the data\n",
    "        verteilung = list(zip(self.both, self.labels))\n",
    "        verteilung = sorted(verteilung,key=lambda x: x[1])\n",
    "\n",
    "        for key, group in groupby(verteilung, lambda x: x[1]):\n",
    "            for thing in group:\n",
    "                print(key,thing)\n",
    "                grouped[key].append(int(thing[0]))\n",
    "\n",
    "        plt.hist(grouped, k, histtype='bar',normed=1, alpha=0.75)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir+fname+str(k))\n",
    "        plt.clf()\n",
    "\n",
    "    def plot_histogram2(self, fname, k):\n",
    "        grouped = [[] for x in range(k)]\n",
    "        hist = [[] for x in range(k)]\n",
    "        # the histogram of the data\n",
    "        verteilung = list(zip(self.both, self.labels))\n",
    "        verteilung = sorted(verteilung, key=lambda x: x[1])\n",
    "\n",
    "        for key, group in groupby(verteilung, lambda x: x[1]):\n",
    "            for thing in group:\n",
    "                #print(key, thing)\n",
    "                grouped[key].append(int(thing[0]))\n",
    "            hist[key].append(np.histogram(grouped[key],bins=2)[0])\n",
    "\n",
    "        guardian = []\n",
    "        sun = []\n",
    "        for cluster in hist:\n",
    "            guardian.append(cluster[0][0])\n",
    "            sun.append(cluster[0][1])\n",
    "\n",
    "        ind = np.arange(k)  # the x locations for the groups\n",
    "        width = 0.35  # the width of the bars\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        rects1 = ax.bar(ind, guardian, width, color='b')\n",
    "        rects2 = ax.bar(ind + width, sun, width, color='y')\n",
    "\n",
    "        # add some text for labels, title and axes ticks\n",
    "        ax.set_ylabel('Frequency in topic clusters')\n",
    "        ax.set_title('Distribution of Guardian/Sun articles in each cluster')\n",
    "        ax.set_xticks(ind + width / 2)\n",
    "        ax.set_xticklabels(range(k))\n",
    "\n",
    "        ax.legend((rects1[0], rects2[0]), ('The Guardian', 'Sun'))\n",
    "        plt.savefig(self.dir + fname + str(k))\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bb83b5e9cb93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mk_means\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlda_Simon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Hennys/Uni/ANLP/final_project/ANLP_Final_Project/k_means.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "more_stopwords = [\"Mrs\", \"says\", \"Mr\"]\n",
    "stopword = stopwords.words(\"english\") + list(string.punctuation) + more_stopwords\n",
    "#print(stopword)\n",
    "filename1 = \"Corpora/filtered/filteredjust_hl_article_guardian_tokenized.txt\"\n",
    "filename2 = \"Corpora/filtered/filteredjust_hl_article_tokenized.txt\"\n",
    "\n",
    "\n",
    "# no_below : No words which appear in less than X articles\n",
    "# no_above : No words which appear in more than X % of the articles\n",
    "lda = LDA(filename1, filename2, stopword, num_topics=7, no_below=20, no_above=0.5)\n",
    "corpus_feature_vectors = lda.corpus_feature_vectors\n",
    "output = lda.final_output\n",
    "\n",
    "#for k in [3,4,5,6,7]:\n",
    "#    k_means = kmeans(output, \"histograms\")\n",
    "#    labels,score = k_means.cluster(k)\n",
    "#    k_means.plot_histogram2(\"histogram\",k)\n",
    "\n",
    "print(lda.get_topics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snt = SentimentIntensityAnalyzer()\n",
    "\n",
    "# polarity score: dict with overall, neg, neu, pos\n",
    "def get_sentiment_score(sentences):\n",
    "\tscores = []\n",
    "\tfor sentence in sentences:\n",
    "\t\tscore = snt.polarity_scores(sentence)\n",
    "\t\t#score_values = [ val for key, val in score.items] not right order\n",
    "\t\tscore_values = []\n",
    "\t\tscore_values.append(str(score[\"compound\"]))\n",
    "\t\tscore_values.append(str(score[\"neg\"]))\n",
    "\t\tscore_values.append(str(score[\"neu\"]))\n",
    "\t\tscore_values.append(str(score[\"pos\"]))\n",
    "\t\tscores.append(score_values)\n",
    "\treturn scores\n",
    "\n",
    "def write_scores_to_file(out_file, scores):\n",
    "\twith open(out_file, \"w\") as o:\n",
    "\t\to.write(\"Overall score\\tNegative\\tNeutral\\tPositive\\n\")\n",
    "\t\tfor score in scores:\n",
    "\t\t\to.write(\"\\t\".join(score))\n",
    "\t\t\to.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of manual and automatic score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_overall_score(scored_file):\n",
    "\toverall_scores = []\n",
    "\twith open(scored_file, 'r') as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\toverall_scores.append(round(float(line.split(\"\\t\")[0]), 1))\n",
    "\treturn overall_scores\n",
    "\n",
    "def read_manual_sa_score(manual_sa_file):\n",
    "\tmanual_scores = []\n",
    "\twith open(manual_sa_file, 'r') as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\tmanual_scores.append(float(line.strip()))\n",
    "\treturn manual_scores\n",
    "\n",
    "def compare_scores(automatic, manual, ok_range):\n",
    "\tright = 0\n",
    "\twrong = 0\n",
    "\twrong_indices = []\n",
    "\tok_range = float(ok_range)\n",
    "\tfor i, (m_score, a_score) in enumerate(zip(automatic, manual)):\n",
    "\t\tif m_score - a_score <= ok_range:\n",
    "\t\t\tright += 1\n",
    "\t\telse:\n",
    "\t\t\twrong += 1\n",
    "\t\t\twrong_indices.append(i)\n",
    "\treturn right, wrong, wrong_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
