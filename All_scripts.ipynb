{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Political Stance Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier kommt nochn schöner Text hin oder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](images/pipeline_ANLP_project.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Simon\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import re, string, gensim, os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from tokenize_articles import read_corpus\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_headlines(cleaned_article, output_file_hl):\n",
    "    '''Extracts just the headline from the raw data and saves it to a text file. \n",
    "    Used in conjunction with the function clean_corpora.'''\n",
    "    \n",
    "    with open(output_file_hl, \"a\") as output_hl:\n",
    "        hl = re.sub(r\"(BYLINE:.*|SECTION:.*|BODY: .*)$\", \"\", cleaned_article)\n",
    "        print(hl.strip(), file=output_hl)\n",
    "        \n",
    "def clean_corpora(input_file, output_file, output_file_headlines=None, guardian=False, sun=False, telegraph=False, guardian_hl=False, \n",
    "                  sun_hl=False, telegraph_hl=False):\n",
    "    '''Takes the raw LexisNexis files as input and extracts the headlines & articles and just headlines (optional), \n",
    "    writes them to a text file. Due to subtle formatting differences between the newspapers, \n",
    "    there are slight differences in the preprocessing'''\n",
    "    \n",
    "    text = open(input_file).read()\n",
    "    split_text = text.strip().split(\"All Rights Reserved\")\n",
    "    \n",
    "    with open(output_file, \"a\") as output:\n",
    "        for un_article in split_text:\n",
    "            un_article1 = un_article.replace(\"\\n\", \" \")\n",
    "            article3 = un_article1.replace(\"\\\\\", \"\")\n",
    "            article2 = re.sub(r\"(BYLINE: .* [0-9]* words|SECTION: .* LETTER|SECTION: .* words)\", \"\", article3)\n",
    "            article = re.sub(r\"(LOAD-DATE:.*|LOAD-DATE)$\", \"\", article2)\n",
    "    \n",
    "            if guardian:          \n",
    "                article_g = re.sub(r\"^(.*GMT)\", \"\", article)\n",
    "                print(article_g.strip(), file=output)\n",
    "                \n",
    "                if guardian_hl:\n",
    "                    article_g1 = re.sub(r\"^(.*GMT)\", \"\", article3)\n",
    "                    extract_headlines(article_g1, output_file_headlines)\n",
    "                               \n",
    "            if sun:\n",
    "                article_s = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;)\", \"\", article)\n",
    "                article_s = re.sub(r\"([a-zA-Z.]*@the[-]*sun\\.co\\.uk.*)$\", \"\", article_s)\n",
    "                print(article_s.strip(), file=output)\n",
    "            \n",
    "                if sun_hl:\n",
    "                    article_s1 = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;)\", \"\", article3)\n",
    "                    extract_headlines(article_s1, output_file_headlines)\n",
    "                    \n",
    "            if telegraph:\n",
    "                article_t1 = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;|Scotland)\", \"\", article)\n",
    "                article_t = re.sub(r\"(Copyright [0-9]+ Telegraph Media Group Limited|BYLINE: .* BODY:|[0-9]* of [0-9]* DOCUMENTS|HEADLINE:)\", \"\", article_t1)\n",
    "                \n",
    "                if telegraph_hl:\n",
    "                    article_t1 = re.sub(r\"(HEADLINE:|Scotland)\", \"\", article_t1)\n",
    "                    extract_headlines(article_t1, output_file_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(input_file):\n",
    "\tcorpus = []\n",
    "\twith open(input_file) as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != '\\n':\n",
    "\t\t\t\tcorpus.append(line.strip())\n",
    "\treturn corpus\n",
    "\n",
    "def tokenize_corpus(articles):\n",
    "\ttokenized_articles = []\n",
    "\tfor sentence in articles:\n",
    "\t\ttokenized_articles.append(word_tokenize(sentence))\n",
    "\treturn tokenized_articles\n",
    "\n",
    "def write_to_file(tokenized_articles, output_file):\n",
    "\twith open(output_file, 'w') as o:\n",
    "\t\tfor article in tokenized_articles:\n",
    "\t\t\to.write(\" \".join(article))\n",
    "\t\t\to.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(input_file):\n",
    "\tsentences = []\n",
    "\twith open(input_file) as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\tsentences.append(line.strip())\n",
    "\treturn sentences\n",
    "\n",
    "def annotate_articles(sentences):\n",
    "\tannotated_articles = []\n",
    "\tfor sentence in sentences:\n",
    "\t\tsentence.lower()\n",
    "\t\tp = Popen(['/Applications/Treetagger/cmd/tree-tagger-english'], stdout=PIPE, stdin=PIPE, stderr=PIPE, encoding=\"utf8\")\n",
    "\t\tout = p.communicate(input=sentence)[0]\n",
    "\t\tarticle = []\n",
    "\t\tannotated_words = out.split(\"\\n\")\n",
    "\t\tfor word_anno in annotated_words:\n",
    "\t\t\tword_anno = word_anno.split(\"\\t\")\n",
    "\t\t\tif len(word_anno) == 3:\n",
    "\t\t\t\tannotation = (word_anno[0], (word_anno[1], word_anno[2]))\n",
    "\t\t\t\tarticle.append(annotation)\n",
    "\t\tannotated_articles.append(article)\n",
    "\treturn annotated_articles\n",
    "\n",
    "def get_lemma(articles):\n",
    "\tlemma_articles = []\n",
    "\tfor article in articles:\n",
    "\t\tlemma_article = []\n",
    "\t\tfor word, anno in article:\n",
    "\t\t\tif anno[1] == \"<unknown>\":\n",
    "\t\t\t\tlemma_article.append(word)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlemma_article.append(anno[1])\n",
    "\t\tlemma_articles.append(lemma_article)\n",
    "\treturn lemma_articles\n",
    "\n",
    "def write_to_file(lemma_articles, out_file):\n",
    "\twith open(out_file, \"w\") as out:\n",
    "\t\tfor article in lemma_articles:\n",
    "\t\t\tout.write(\" \".join(article))\n",
    "\t\t\tout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(processed_file, output_file):\n",
    "    '''Takes the lemmatized, tokenized file as input and removes punctuation, stopwords and other strangely formatted\n",
    "    words/punctuation'''\n",
    "    \n",
    "    stop_words_nltk = list(stopwords.words('english'))\n",
    "    to_delete = [\"brexit\", \"``\", \"''\", \"'s\", \"·\", \"wo\", \"n't\", \"...\", \"@card@\"]\n",
    "    \n",
    "    stop_words = stop_words_nltk + to_delete + list(string.punctuation)\n",
    "\n",
    "    with open(output_file, \"w\") as output:\n",
    "        to_be_filtered = open(processed_file).readlines()\n",
    "        corpus_list = [line.strip() for line in to_be_filtered]\n",
    "        corpus_nlist = [each_word.split() for each_word in corpus_list]\n",
    "        for sentence in range(0, len(corpus_nlist)):\n",
    "            for word in corpus_nlist[sentence]:\n",
    "                word = re.sub(r\"^(\\')\", \"\", word) #getting rid of the ' at the beginning of some words\n",
    "                if word.lower() not in stop_words:\n",
    "                    output.write(word)\n",
    "                    output.write(\" \")\n",
    "            output.write(\"\\n\") #line break after each article/hl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    '''\n",
    "    This class implements a Latent Dirichlet Allocation. Initializing one instance creates a\n",
    "    model and saves it to a folder. It also uses this model to create a topic distribution\n",
    "    vector for every article in the corpora\n",
    "    '''\n",
    "    def __init__(self, filename1, filename2, num_topics=10, no_below=20, no_above=0.5):\n",
    "        self.num_topics = num_topics\n",
    "        self.corpus = self.createCorpus(filename1, filename2)\n",
    "        self.model_corpus, self.dictionary = self.createModelCorpus(no_below, no_above)\n",
    "        try:\n",
    "            self.model = gensim.models.LdaModel.load(\"LDA_Models/ldamodel_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above))\n",
    "        except:\n",
    "            self.model = self.train_lda()\n",
    "            self.model.save(\"LDA_Models/ldamodel_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above))\n",
    "\n",
    "        self.corpus_feature_vectors = self.apply_lda()\n",
    "        self.final_output = self.createFinalOutput()\n",
    "\n",
    "    def createCorpus(self, filename1, filename2):\n",
    "        ''' \n",
    "        Creating lists of tokens in a list of articles for further processing\n",
    "        '''\n",
    "        with open(filename1,encoding=\"utf-8\") as f1:\n",
    "            with open(filename2, encoding=\"utf-8\") as f2:\n",
    "                all_articles = f1.readlines()\n",
    "                all_articles.extend(f2.readlines())\n",
    "                corpus = [[token for token in article.strip().split(\" \")] for article in all_articles]\n",
    "        return corpus\n",
    "\n",
    "    def createModelCorpus(self, no_below, no_above):\n",
    "        ''' \n",
    "        Creating a model corpus and a dictionary for the lda model\n",
    "        ''' \n",
    "        dictionary = corpora.Dictionary(self.corpus)\n",
    "        dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "        model_corpus = [dictionary.doc2bow(token) for token in self.corpus]\n",
    "        return model_corpus, dictionary\n",
    "\n",
    "    def train_lda(self):\n",
    "        ''' \n",
    "        Creating a LDA model from the whole corpus\n",
    "        ''' \n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(self.model_corpus, num_topics=self.num_topics, id2word=self.dictionary, passes=10)\n",
    "        return ldamodel\n",
    "\n",
    "    def apply_lda(self):\n",
    "        ''' \n",
    "        applying the LDA model to all articles to get the feature vectors\n",
    "        ''' \n",
    "        corpus_feature_vectors = []\n",
    "        for article in self.model_corpus:\n",
    "            corpus_feature_vectors.append(self.model[article])\n",
    "        return corpus_feature_vectors\n",
    "\n",
    "    def createFinalOutput(self):\n",
    "        ''' \n",
    "        arrange the feature vectors in a list\n",
    "        ''' \n",
    "        output = []\n",
    "        for vec in self.corpus_feature_vectors:\n",
    "            vec_dic = dict(vec)\n",
    "            output.append([(vec_dic[topic] if topic in vec_dic else 0) for topic in range(10)])\n",
    "        return output\n",
    "\n",
    "    def get_topics(self):\n",
    "        ''' \n",
    "        show the topics\n",
    "        ''' \n",
    "        return(self.model.show_topics(num_topics=12,num_words=5))\n",
    "\n",
    "filename1 = \"Corpora/filtered/filteredjust_hl_article_guardian_lemmatized.txt\"\n",
    "filename2 = \"Corpora/filtered/filteredhl_article_tele_lemmatized.txt\"\n",
    "\n",
    "\n",
    "#k_means = kmeans(output,str(num_topics)+\"_\"+str(no_above))\n",
    "#k_means.plot_elbow(range(2, 10), \"Elbow_plot\", individ_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1158 is out of bounds for axis 1 with size 1158",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ee607bf7122a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilename2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Corpora/filtered/filteredhl_article_tele_lemmatized.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_below\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_above\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcorpus_feature_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_feature_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f425ff3e7cb2>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename1, filename2, num_topics, no_below, no_above)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LDA_Models/ldamodel_topics=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_no_above=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_above\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_feature_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_lda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateFinalOutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f425ff3e7cb2>\u001b[0m in \u001b[0;36mapply_lda\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mcorpus_feature_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_corpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mcorpus_feature_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcorpus_feature_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, bow, eps)\u001b[0m\n\u001b[0;32m   1185\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_probability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \"\"\"\n\u001b[1;32m-> 1187\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dispatcher'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mget_document_topics\u001b[1;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[0;32m   1022\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m         \u001b[0mtopic_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# normalize distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    506\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m             \u001b[0mexpElogbetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m             \u001b[1;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1158 is out of bounds for axis 1 with size 1158"
     ]
    }
   ],
   "source": [
    "filename1 = \"Corpora/filtered/filteredjust_hl_article_guardian_lemmatized.txt\"\n",
    "filename2 = \"Corpora/filtered/filteredhl_article_tele_lemmatized.txt\"\n",
    "\n",
    "lda = LDA(filename1, filename2, num_topics=12, no_below=20, no_above=0.7)\n",
    "\n",
    "corpus_feature_vectors = lda.corpus_feature_vectors\n",
    "output = lda.final_output\n",
    "lda.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3281267fe026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "class kmeans():\n",
    "\n",
    "    def __init__(self,feature_vector,image_dir,r_state=42):\n",
    "\n",
    "        self.feature_vector = feature_vector\n",
    "        self.r_state = r_state\n",
    "        self.dir = \"cluster_images/\"+image_dir+\"/\"\n",
    "        guardian = np.ones(998)\n",
    "        sun = np.zeros(876)\n",
    "        self.both = np.concatenate([guardian, sun])\n",
    "\n",
    "\n",
    "    def cluster(self,k):\n",
    "        clustering = KMeans(n_clusters=k,random_state=self.r_state)\n",
    "        labels = clustering.fit_predict(self.feature_vector)\n",
    "        self.labels = labels\n",
    "        self.centroids = clustering.cluster_centers_\n",
    "        return(labels,clustering.inertia_)\n",
    "\n",
    "    def visualize_data(self,fname):\n",
    "\n",
    "        X_reduced = PCA(n_components=2).fit_transform(self.feature_vector)\n",
    "        fig = plt.figure()\n",
    "\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=self.labels,marker=\"^\", s=both*10, edgecolor=\"red\", linewidth=0.3)\n",
    "        ax.scatter(X_reduced[:,0], X_reduced[:,1], c=self.labels,marker=\"o\", s=((both-1)*-1)*10, edgecolor=\"black\", linewidth=0.3)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        #plt.colorbar(scatter)\n",
    "\n",
    "        if not os.path.exists(self.dir):\n",
    "            os.makedirs(self.dir)\n",
    "        plt.savefig(self.dir+fname,dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_elbow(self,k_range,fname,individ_plot=False):\n",
    "        distorsions = []\n",
    "        for k in k_range:\n",
    "            labels, score = self.cluster(k)\n",
    "            distorsions.append(score)\n",
    "            if individ_plot:\n",
    "                self.visualize_data(\"cluster_\"+str(k))\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.xlabel(\"number of clusters: k\")\n",
    "        plt.ylabel(\"Sum of squared distances of samples to their closest cluster center.\")\n",
    "        plt.plot(k_range, distorsions)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(self.dir+fname,dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_histogram(self, fname, k):\n",
    "        #does not work\n",
    "        grouped = [[] for x in range(k)]\n",
    "        # the histogram of the data\n",
    "        verteilung = list(zip(self.both, self.labels))\n",
    "        verteilung = sorted(verteilung,key=lambda x: x[1])\n",
    "\n",
    "        for key, group in groupby(verteilung, lambda x: x[1]):\n",
    "            for thing in group:\n",
    "                print(key,thing)\n",
    "                grouped[key].append(int(thing[0]))\n",
    "\n",
    "        plt.hist(grouped, k, histtype='bar',normed=1, alpha=0.75)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir+fname+str(k))\n",
    "        plt.clf()\n",
    "\n",
    "    def plot_histogram2(self, fname, k):\n",
    "        grouped = [[] for x in range(k)]\n",
    "        hist = [[] for x in range(k)]\n",
    "        # the histogram of the data\n",
    "        verteilung = list(zip(self.both, self.labels))\n",
    "        verteilung = sorted(verteilung, key=lambda x: x[1])\n",
    "\n",
    "        for key, group in groupby(verteilung, lambda x: x[1]):\n",
    "            for thing in group:\n",
    "                #print(key, thing)\n",
    "                grouped[key].append(int(thing[0]))\n",
    "            hist[key].append(np.histogram(grouped[key],bins=2)[0])\n",
    "\n",
    "        guardian = []\n",
    "        sun = []\n",
    "        for cluster in hist:\n",
    "            guardian.append(cluster[0][0])\n",
    "            sun.append(cluster[0][1])\n",
    "\n",
    "        ind = np.arange(k)  # the x locations for the groups\n",
    "        width = 0.35  # the width of the bars\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        rects1 = ax.bar(ind, guardian, width, color='b')\n",
    "        rects2 = ax.bar(ind + width, sun, width, color='y')\n",
    "\n",
    "        # add some text for labels, title and axes ticks\n",
    "        ax.set_ylabel('Frequency in topic clusters')\n",
    "        ax.set_title('Distribution of Guardian/Sun articles in each cluster')\n",
    "        ax.set_xticks(ind + width / 2)\n",
    "        ax.set_xticklabels(range(k))\n",
    "\n",
    "        ax.legend((rects1[0], rects2[0]), ('The Guardian', 'Sun'))\n",
    "        plt.savefig(self.dir + fname + str(k))\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bb83b5e9cb93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mk_means\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlda_Simon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Hennys/Uni/ANLP/final_project/ANLP_Final_Project/k_means.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "more_stopwords = [\"Mrs\", \"says\", \"Mr\"]\n",
    "stopword = stopwords.words(\"english\") + list(string.punctuation) + more_stopwords\n",
    "#print(stopword)\n",
    "filename1 = \"Corpora/filtered/filteredjust_hl_article_guardian_tokenized.txt\"\n",
    "filename2 = \"Corpora/filtered/filteredjust_hl_article_tokenized.txt\"\n",
    "\n",
    "\n",
    "# no_below : No words which appear in less than X articles\n",
    "# no_above : No words which appear in more than X % of the articles\n",
    "lda = LDA(filename1, filename2, stopword, num_topics=7, no_below=20, no_above=0.5)\n",
    "corpus_feature_vectors = lda.corpus_feature_vectors\n",
    "output = lda.final_output\n",
    "\n",
    "#for k in [3,4,5,6,7]:\n",
    "#    k_means = kmeans(output, \"histograms\")\n",
    "#    labels,score = k_means.cluster(k)\n",
    "#    k_means.plot_histogram2(\"histogram\",k)\n",
    "\n",
    "print(lda.get_topics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snt = SentimentIntensityAnalyzer()\n",
    "\n",
    "# polarity score: dict with overall, neg, neu, pos\n",
    "def get_sentiment_score(sentences):\n",
    "\tscores = []\n",
    "\tfor sentence in sentences:\n",
    "\t\tscore = snt.polarity_scores(sentence)\n",
    "\t\t#score_values = [ val for key, val in score.items] not right order\n",
    "\t\tscore_values = []\n",
    "\t\tscore_values.append(str(score[\"compound\"]))\n",
    "\t\tscore_values.append(str(score[\"neg\"]))\n",
    "\t\tscore_values.append(str(score[\"neu\"]))\n",
    "\t\tscore_values.append(str(score[\"pos\"]))\n",
    "\t\tscores.append(score_values)\n",
    "\treturn scores\n",
    "\n",
    "def write_scores_to_file(out_file, scores):\n",
    "\twith open(out_file, \"w\") as o:\n",
    "\t\to.write(\"Overall score\\tNegative\\tNeutral\\tPositive\\n\")\n",
    "\t\tfor score in scores:\n",
    "\t\t\to.write(\"\\t\".join(score))\n",
    "\t\t\to.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of manual and automatic score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_overall_score(scored_file):\n",
    "\toverall_scores = []\n",
    "\twith open(scored_file, 'r') as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\toverall_scores.append(round(float(line.split(\"\\t\")[0]), 1))\n",
    "\treturn overall_scores\n",
    "\n",
    "def read_manual_sa_score(manual_sa_file):\n",
    "\tmanual_scores = []\n",
    "\twith open(manual_sa_file, 'r') as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\tmanual_scores.append(float(line.strip()))\n",
    "\treturn manual_scores\n",
    "\n",
    "def compare_scores(automatic, manual, ok_range):\n",
    "\tright = 0\n",
    "\twrong = 0\n",
    "\twrong_indices = []\n",
    "\tok_range = float(ok_range)\n",
    "\tfor i, (m_score, a_score) in enumerate(zip(automatic, manual)):\n",
    "\t\tif m_score - a_score <= ok_range:\n",
    "\t\t\tright += 1\n",
    "\t\telse:\n",
    "\t\t\twrong += 1\n",
    "\t\t\twrong_indices.append(i)\n",
    "\treturn right, wrong, wrong_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
