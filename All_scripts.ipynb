{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Political Stance Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A combination of topic modelling techniques and sentiment analysis applied to British newspaper articles written about the topic Brexit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](images/pipeline_ANLP_project.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tim\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "c:\\users\\tim\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import re, string, gensim, os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our data suitable for use with the topic modelling techniques & sentiment analysis, we first extract the headlines and articles, using regular expressions. This removes all metadata that is included in the downloads from LexisNexis. We then tokenise and lemmatise the articles and remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data\n",
      "\n",
      "\n",
      "                              1 of 1000 DOCUMENTS\n",
      "\n",
      "\n",
      "\n",
      "                                  The Guardian\n",
      "\n",
      "                     December 1, 2016 Thursday 8:18 PM GMT\n",
      "\n",
      "It won't be easy to stop Brexit. But here are four ways to do it;\n",
      "Chip away, every day. This is a long game but, as harsh reality bites, time will\n",
      "be on the side of the remainers\n",
      "\n",
      "BYLINE: Martin Kettle\n",
      "\n",
      "SECTION: OPINION\n",
      "\n",
      "LENGTH: 912 words\n",
      "\n",
      "\n",
      "Those of us with only a smattering of knowledge about the ancient world know one\n",
      "thing about Cato the Elder. During Rome's long wars against Hannibal, Cato ended\n",
      "every speech in the senate with the same words: \"Carthage must be destroyed.\"\n",
      "\n",
      "\" Brexit must be stopped\" is unlikely to last as long as Cato's catchphrase has\n",
      "managed to. But it focuses the mind. Those who think Brexit must be stopped are\n",
      "not the majority. But they have a case and a cause, and they are right. So how\n",
      "might stoppage be achieved?\n",
      "\n",
      "Probably not by a political movement headed by Tony Blair. The former prime\n",
      "minister is not heading back into frontline politics. But he is one of the\n",
      "biggest names to insist that Brexit is not yet irrevocable. He told the New\n",
      "Statesman last week that Brexit \" can be stopped if the British people decide\n",
      "that, having seen what it means, the pain-gain cost-benefit analysis doesn't\n",
      "stack up\". And on that he is absolutely right.\n",
      "\n",
      "When inflation rises and growth slows next year, make sure Brexit's role is\n",
      "clearly spelled out\n",
      "\n",
      "Blair carries so much baggage that it is inconceivable he either could or should\n",
      "play the leading role in any campaign. The Iraq war was wrong and it is no part\n",
      "of my argument that the past can be brushed aside. But Blair has serious things\n",
      "to say about Brexit that serious people ought to listen to. It's time his\n",
      "critics were big enough to give him a break. The 439-70 House of Commons vote\n",
      "this week against the SNP's effort to sanction Blair for the events of 2003 may\n",
      "suggest there is some space for the former Labour leader to at least be heard.\n",
      "But don't hold your breath.\n",
      "\n",
      "If Brexit is to be stopped it will require time, a change of public mood, and an\n",
      "alliance. The SNP, Greens, Liberal Democrats, significant parts of the Labour\n",
      "party and a minority of Tories would all have roles to play. People from outside\n",
      "politics are crucial too. This week's joint press conference by Nick Clegg,\n",
      "Chuka Umunna and Anna Soubry was a start. But as Clegg says, to turn the\n",
      "referendum around needs the people's consent, not a procedural trick. This will\n",
      "only happen if the public mood changes. Anti-Brexit campaigners should try to\n",
      "change it, and here's how.\n",
      "\n",
      "The first point is to be clear who they need to be talking to, rather than\n",
      "squabbling about which of them is entitled to do the talking. The name of this\n",
      "game is changing minds. So there is absolutely no point crafting a campaign that\n",
      "is aimed at fundamentalist Eurosceptics who never wanted Britain to be part of\n",
      "the EU in the first place. Nor is there any point in focusing on racists and\n",
      "xenophobes. People who don't like foreigners or people with coloured skin are\n",
      "not going to change their minds.\n",
      "\n",
      "But that leaves a lot of people who voted for Brexit, in hundreds of thousands\n",
      "of cases, on the basis of their own experience. They were sold a false\n",
      "prospectus by the leave campaign; but they need a better one, which offers them\n",
      "hope, support, material improvements in their lives, and confidence. Theresa May\n",
      "gets this, though she is doomed to promote Brexit and not to stop it. Those who\n",
      "want to stop Brexit need to learn this lesson: the aim of any campaign must be\n",
      "to persuade these voters that there is a better way of getting the things they\n",
      "want than leaving the EU. Don't berate, persuade. And get out of the bubble.\n",
      "\n",
      "The second key principle is to accept that this is a long game. Brexit won in\n",
      "June 2016. It won't be turned around quickly. Stopping Brexit is on the margins\n",
      "of political possibility right now. It could be that Brexit will be slowed by\n",
      "the supreme court's ruling, due in January. But that's just the start. Opponents\n",
      "of Brexit should settle in. Time is on their side.\n",
      "\n",
      "The negotiations with the EU will take a minimum of two years - longer if there\n",
      "is a transitional phase. The pressure to bring things to a head will be enormous\n",
      "and will grow, both in Britain and in the other 27 EU countries. British\n",
      "opponents of Brexit must be EU reformers too.\n",
      "\n",
      "Related:  Tony Blair: Brexit could be stopped if Britons change their minds\n",
      "\n",
      "The third point is to remember Cato. Chip away, every day. Every time something\n",
      "new and troubling happens, make it clear that things would be different if\n",
      "Brexit were stopped. This week's immigration figures showed a pre-referendum\n",
      "surge. Without Brexit this wouldn't have happened. Hate crimes have\n",
      "proliferated. Brexit shares the blame for that. When inflation rises and growth\n",
      "slows next year, make sure Brexit's role is spelled out. If ministers abandon\n",
      "the single market in favour of migration curbs, make Brexit's responsibility\n",
      "clear. Unless anti-Brexit campaigners have established in the public mind that\n",
      "there is a clear and viable no-Brexit alternative, they won't be in a position\n",
      "to make the most of their opportunities.\n",
      "\n",
      "The fourth point is the other side of the same coin. The leave campaign lied\n",
      "through its teeth about the benefits of Brexit. It said there would be Â£350m\n",
      "extra every week for the NHS. Last week the chancellor said precisely nothing\n",
      "about any extra NHS spending in the next four years. And look what is actually\n",
      "happening to the NHS. The leave campaign landed the May government with a huge\n",
      "promise that it cannot deliver. The opposition parties need to link the two at\n",
      "every opportunity.\n",
      "\n",
      "Stopping Brexit will not be easy. Recovering from a big defeat is hard. The\n",
      "campaign is more likely to fail than to succeed. The Brexiteers will fight very\n",
      "dirty. But the prize is immense - and Hannibal was defeated in a day.\n",
      "\n",
      "LOAD-DATE: December 1, 2016\n",
      "\n",
      "LANGUAGE: ENGLISH\n",
      "\n",
      "PUBLICATION-TYPE: Newspaper\n",
      "\n",
      "JOURNAL-CODE: WEBGNS\n",
      "\n",
      "\n",
      "  Copyright 2016 The Guardian, a division of Transcontinental Media Group Inc.\n",
      "                              All Rights Reserved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"sample_for_notebook.txt\") as f_raw:\n",
    "    print(\"Raw data\\n\")\n",
    "    print(f_raw.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_headlines(cleaned_article_list, output_file_hl):\n",
    "    '''Extracts just the headline from the raw data and saves it to a text file. \n",
    "    Used in conjunction with the function clean_corpora.'''\n",
    "    \n",
    "    open(output_file_hl, 'w').close()\n",
    "    \n",
    "    with open(output_file_hl, \"a\") as output_hl:\n",
    "        for cleaned_hl in cleaned_article_list:\n",
    "            hl = re.sub(r\"(BYLINE:.*|SECTION:.*|BODY: .*)$\", \"\", cleaned_hl)\n",
    "            print(hl.strip(), file=output_hl)\n",
    "        \n",
    "def clean_corpora(input_file, output_file, output_file_headlines=None, guardian=False, sun=False, telegraph=False, guardian_hl=False, \n",
    "                  sun_hl=False, telegraph_hl=False):\n",
    "    '''Takes the raw LexisNexis files as input and extracts the headlines & articles and just headlines (optional), \n",
    "    writes them to a text file. Due to subtle formatting differences between the newspapers, \n",
    "    there are slight differences in the preprocessing'''\n",
    "    \n",
    "    open(output_file, 'w').close() #clears output file\n",
    "    g_for_hl, s_for_hl, t_for_hl = [], [], []\n",
    "    g_hl, t_hl, s_hl = [], [], []\n",
    "    \n",
    "    text = open(input_file).read()\n",
    "    split_text = text.strip().split(\"All Rights Reserved\")\n",
    "    \n",
    "    with open(output_file, \"a\") as output:\n",
    "        for un_article in split_text:\n",
    "            un_article1 = un_article.replace(\"\\n\", \" \")\n",
    "            article3 = un_article1.replace(\"\\\\\", \"\")\n",
    "            article2 = re.sub(r\"(BYLINE: .* [0-9]* words|SECTION: .* LETTER|SECTION: .* words)\", \"\", article3)\n",
    "            article = re.sub(r\"(LOAD-DATE:.*|LOAD-DATE)$\", \"\", article2)\n",
    "            g_for_hl.append(article3)\n",
    "            s_for_hl.append(article3)\n",
    "            \n",
    "    \n",
    "            if guardian:          \n",
    "                article_g = re.sub(r\"^(.*GMT)\", \"\", article)\n",
    "                print(article_g.strip(), file=output)\n",
    "                \n",
    "            if sun:\n",
    "                article_s = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;)\", \"\", article)\n",
    "                article_s = re.sub(r\"([a-zA-Z.]*@the[-]*sun\\.co\\.uk.*)$\", \"\", article_s)\n",
    "                print(article_s.strip(), file=output)\n",
    "                \n",
    "            if telegraph:\n",
    "                article_t1 = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;|Scotland)\", \"\", article)\n",
    "                t_for_hl.append(article_t1)\n",
    "                article_t = re.sub(r\"(Copyright [0-9]+ Telegraph Media Group Limited|BYLINE: .* BODY:|[0-9]* of [0-9]* DOCUMENTS|HEADLINE:)\", \"\", article_t1)\n",
    "                print(article_t.strip(), file=output)\n",
    "                \n",
    "        if guardian_hl:\n",
    "            for pre_hl in g_for_hl:\n",
    "                article_g1 = re.sub(r\"^(.*GMT)\", \"\", pre_hl)\n",
    "                g_hl.append(article_g1)\n",
    "            extract_headlines(g_hl, output_file_headlines)\n",
    "\n",
    "        if sun_hl:\n",
    "            for pre_hl in s_for_hl:\n",
    "                article_s1 = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;)\", \"\", pre_hl)\n",
    "                s_hl.append(article_s1)\n",
    "            extract_headlines(s_hl, output_file_headlines)\n",
    "   \n",
    "        if telegraph_hl:\n",
    "            for pre_hl in t_for_hl:\n",
    "                article_t1 = re.sub(r\"(HEADLINE:|Scotland)\", \"\", pre_hl)\n",
    "                t_hl.append(article_t1)\n",
    "            extract_headlines(article_t1, output_file_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline and article\n",
      "\n",
      "It won't be easy to stop Brexit. But here are four ways to do it; Chip away, every day. This is a long game but, as harsh reality bites, time will be on the side of the remainers     Those of us with only a smattering of knowledge about the ancient world know one thing about Cato the Elder. During Rome's long wars against Hannibal, Cato ended every speech in the senate with the same words: \"Carthage must be destroyed.\"  \" Brexit must be stopped\" is unlikely to last as long as Cato's catchphrase has managed to. But it focuses the mind. Those who think Brexit must be stopped are not the majority. But they have a case and a cause, and they are right. So how might stoppage be achieved?  Probably not by a political movement headed by Tony Blair. The former prime minister is not heading back into frontline politics. But he is one of the biggest names to insist that Brexit is not yet irrevocable. He told the New Statesman last week that Brexit \" can be stopped if the British people decide that, having seen what it means, the pain-gain cost-benefit analysis doesn't stack up\". And on that he is absolutely right.  When inflation rises and growth slows next year, make sure Brexit's role is clearly spelled out  Blair carries so much baggage that it is inconceivable he either could or should play the leading role in any campaign. The Iraq war was wrong and it is no part of my argument that the past can be brushed aside. But Blair has serious things to say about Brexit that serious people ought to listen to. It's time his critics were big enough to give him a break. The 439-70 House of Commons vote this week against the SNP's effort to sanction Blair for the events of 2003 may suggest there is some space for the former Labour leader to at least be heard. But don't hold your breath.  If Brexit is to be stopped it will require time, a change of public mood, and an alliance. The SNP, Greens, Liberal Democrats, significant parts of the Labour party and a minority of Tories would all have roles to play. People from outside politics are crucial too. This week's joint press conference by Nick Clegg, Chuka Umunna and Anna Soubry was a start. But as Clegg says, to turn the referendum around needs the people's consent, not a procedural trick. This will only happen if the public mood changes. Anti-Brexit campaigners should try to change it, and here's how.  The first point is to be clear who they need to be talking to, rather than squabbling about which of them is entitled to do the talking. The name of this game is changing minds. So there is absolutely no point crafting a campaign that is aimed at fundamentalist Eurosceptics who never wanted Britain to be part of the EU in the first place. Nor is there any point in focusing on racists and xenophobes. People who don't like foreigners or people with coloured skin are not going to change their minds.  But that leaves a lot of people who voted for Brexit, in hundreds of thousands of cases, on the basis of their own experience. They were sold a false prospectus by the leave campaign; but they need a better one, which offers them hope, support, material improvements in their lives, and confidence. Theresa May gets this, though she is doomed to promote Brexit and not to stop it. Those who want to stop Brexit need to learn this lesson: the aim of any campaign must be to persuade these voters that there is a better way of getting the things they want than leaving the EU. Don't berate, persuade. And get out of the bubble.  The second key principle is to accept that this is a long game. Brexit won in June 2016. It won't be turned around quickly. Stopping Brexit is on the margins of political possibility right now. It could be that Brexit will be slowed by the supreme court's ruling, due in January. But that's just the start. Opponents of Brexit should settle in. Time is on their side.  The negotiations with the EU will take a minimum of two years - longer if there is a transitional phase. The pressure to bring things to a head will be enormous and will grow, both in Britain and in the other 27 EU countries. British opponents of Brexit must be EU reformers too.  Related:  Tony Blair: Brexit could be stopped if Britons change their minds  The third point is to remember Cato. Chip away, every day. Every time something new and troubling happens, make it clear that things would be different if Brexit were stopped. This week's immigration figures showed a pre-referendum surge. Without Brexit this wouldn't have happened. Hate crimes have proliferated. Brexit shares the blame for that. When inflation rises and growth slows next year, make sure Brexit's role is spelled out. If ministers abandon the single market in favour of migration curbs, make Brexit's responsibility clear. Unless anti-Brexit campaigners have established in the public mind that there is a clear and viable no-Brexit alternative, they won't be in a position to make the most of their opportunities.  The fourth point is the other side of the same coin. The leave campaign lied through its teeth about the benefits of Brexit. It said there would be Â£350m extra every week for the NHS. Last week the chancellor said precisely nothing about any extra NHS spending in the next four years. And look what is actually happening to the NHS. The leave campaign landed the May government with a huge promise that it cannot deliver. The opposition parties need to link the two at every opportunity.  Stopping Brexit will not be easy. Recovering from a big defeat is hard. The campaign is more likely to fail than to succeed. The Brexiteers will fight very dirty. But the prize is immense - and Hannibal was defeated in a day.\n",
      "\n",
      "\n",
      "\n",
      "Headline\n",
      "\n",
      "It won't be easy to stop Brexit. But here are four ways to do it; Chip away, every day. This is a long game but, as harsh reality bites, time will be on the side of the remainers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_corpora(\"sample_for_notebook.txt\", \"sample_output.txt\", \"sample_output_headlines.txt\",\n",
    "             guardian=True, guardian_hl=True)\n",
    "\n",
    "with open(\"sample_output.txt\") as f, open(\"sample_output_headlines.txt\") as f_hl:\n",
    "    print(\"\\nHeadline and article\\n\")\n",
    "    print(f.read())\n",
    "    print(\"\\nHeadline\\n\")\n",
    "    print(f_hl.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the corpus, one headline/article per line\n",
    "# returns an array of headlines/articles\n",
    "def read_corpus(input_file):\n",
    "\tcorpus = []\n",
    "\twith open(input_file) as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != '\\n':\n",
    "\t\t\t\tcorpus.append(line.strip())\n",
    "\treturn corpus\n",
    "\n",
    "# the tokenizer provided by nltk is used \n",
    "# returns a list of tokenized headlines/articles \n",
    "def tokenize_corpus(articles):\n",
    "\ttokenized_articles = []\n",
    "\tfor sentence in articles:\n",
    "\t\ttokenized_articles.append(word_tokenize(sentence))\n",
    "\treturn tokenized_articles\n",
    "\n",
    "# Writes one tokenized article per line in a file\n",
    "def write_to_file(tokenized_articles, output_file):\n",
    "\twith open(output_file, 'w') as o:\n",
    "\t\tfor article in tokenized_articles:\n",
    "\t\t\to.write(\" \".join(article))\n",
    "\t\t\to.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'wo', \"n't\", 'be', 'easy', 'to', 'stop', 'Brexit', '.', 'But', 'here', 'are', 'four', 'ways', 'to', 'do', 'it', ';', 'Chip', 'away', ',', 'every', 'day', '.', 'This', 'is', 'a', 'long', 'game', 'but', ',', 'as', 'harsh', 'reality', 'bites', ',', 'time', 'will', 'be', 'on', 'the', 'side', 'of', 'the', 'remainers']]\n"
     ]
    }
   ],
   "source": [
    "sample_corpus = read_corpus(\"sample_output_headlines.txt\")\n",
    "tokenized_sample = tokenize_corpus(sample_corpus)\n",
    "write_to_file(tokenized_sample, \"tokenized_output_file.txt\")\n",
    "print(tokenized_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in the corpus, one headline/article per line\n",
    "# returns an array of headlines/articles\n",
    "def get_sentences(input_file):\n",
    "\tsentences = []\n",
    "\twith open(input_file) as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\tsentences.append(line.strip())\n",
    "\treturn sentences\n",
    "\n",
    "# the headlines/articles are lemmatized using the Treetagger\n",
    "# returns a list of lists with the original word, the POS-Tag and the lemma per article\n",
    "def annotate_articles(sentences):\n",
    "\tannotated_articles = []\n",
    "\tfor sentence in sentences:\n",
    "\t\tsentence.lower()\n",
    "\t\tp = Popen(['/Applications/Treetagger/cmd/tree-tagger-english'], stdout=PIPE, stdin=PIPE, stderr=PIPE, encoding=\"utf8\")\n",
    "\t\tout = p.communicate(input=sentence)[0]\n",
    "\t\tarticle = []\n",
    "\t\tannotated_words = out.split(\"\\n\")\n",
    "\t\tfor word_anno in annotated_words:\n",
    "\t\t\tword_anno = word_anno.split(\"\\t\")\n",
    "\t\t\tif len(word_anno) == 3:\n",
    "\t\t\t\tannotation = (word_anno[0], (word_anno[1], word_anno[2]))\n",
    "\t\t\t\tarticle.append(annotation)\n",
    "\t\tannotated_articles.append(article)\n",
    "\treturn annotated_articles\n",
    "\n",
    "# for each word in an article the lemma is extracted\n",
    "# if lemma is unknown, the original word is chosen\n",
    "# return a list of lemmatized articles\n",
    "def get_lemma(articles):\n",
    "\tlemma_articles = []\n",
    "\tfor article in articles:\n",
    "\t\tlemma_article = []\n",
    "\t\tfor word, anno in article:\n",
    "\t\t\tif anno[1] == \"<unknown>\":\n",
    "\t\t\t\tlemma_article.append(word)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlemma_article.append(anno[1])\n",
    "\t\tlemma_articles.append(lemma_article)\n",
    "\treturn lemma_articles\n",
    "\n",
    "# Writes one lemmatized article per line in a file\n",
    "def write_to_file(lemma_articles, out_file):\n",
    "\twith open(out_file, \"w\") as out:\n",
    "\t\tfor article in lemma_articles:\n",
    "\t\t\tout.write(\" \".join(article))\n",
    "\t\t\tout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Das System kann die angegebene Datei nicht finden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-432df58c34fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlemmatized_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenized_output_file.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mannotated_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mannotate_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlemmatised_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotated_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwrite_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatised_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lemmatised_output_file.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatised_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-1c21d4b1c785>\u001b[0m in \u001b[0;36mannotate_articles\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'/Applications/Treetagger/cmd/tree-tagger-english'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tim\\appdata\\local\\programs\\python\\python36\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[0;32m    707\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    710\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tim\\appdata\\local\\programs\\python\\python36\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m    995\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m    998\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Das System kann die angegebene Datei nicht finden"
     ]
    }
   ],
   "source": [
    "lemmatized_sentences = get_sentences(\"tokenized_output_file.txt\")\n",
    "annotated_sample = annotate_articles(lemmatized_sentences)\n",
    "lemmatised_sample = get_lemma(annotated_sample)\n",
    "write_to_file(lemmatised_sample, \"lemmatised_output_file.txt\")\n",
    "print(lemmatised_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(processed_file, output_file):\n",
    "    '''Takes the lemmatized, tokenized file as input and removes punctuation, stopwords and other strangely formatted\n",
    "    words/punctuation'''\n",
    "    \n",
    "    stop_words_nltk = list(stopwords.words('english'))\n",
    "    to_delete = [\"brexit\", \"``\", \"''\", \"'s\", \"·\", \"wo\", \"n't\", \"...\", \"@card@\"]\n",
    "    \n",
    "    stop_words = stop_words_nltk + to_delete + list(string.punctuation)\n",
    "\n",
    "    with open(output_file, \"w\") as output:\n",
    "        to_be_filtered = open(processed_file).readlines()\n",
    "        corpus_list = [line.strip() for line in to_be_filtered]\n",
    "        corpus_nlist = [each_word.split() for each_word in corpus_list]\n",
    "        for sentence in range(0, len(corpus_nlist)):\n",
    "            for word in corpus_nlist[sentence]:\n",
    "                word = re.sub(r\"^(\\')\", \"\", word) #getting rid of the ' at the beginning of some words\n",
    "                if word.lower() not in stop_words:\n",
    "                    output.write(word)\n",
    "                    output.write(\" \")\n",
    "            output.write(\"\\n\") #line break after each article/hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stop_words(\"lemmatised_output_file.txt\", \"filtered_output_file.txt\")\n",
    "with open(\"filtered_output_file.txt\") as filtered:\n",
    "    print(filtered.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an interpretation of the content of the newspaper articles, we apply topic modeling to them. \n",
    "We use a Latent Dirichlet Allocation (LDA) model from the package gensim. \n",
    "This model assumes that our articles were written using a \"generative process\" in which every word of the article is created out of a distribution of words within a topic. \n",
    "In a first step, this distribution is created by pre-defining a number of topics and then building a model over the whole corpus. In a second step, the model is applied to the articles to find the specific topic mixture which best represents the article. These topic mixtures make up our feature vectors for the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LDA():\n",
    "    def __init__(self, filename1, filename2, stopwords, num_topics=10, no_below=20, no_above=0.5):\n",
    "        self.num_topics = num_topics\n",
    "        self.corpus = self.createCorpus(filename1, filename2, stopwords)\n",
    "        self.model_corpus, self.dictionary = self.createModelCorpus(no_below, no_above)\n",
    "        try:\n",
    "            self.model = gensim.models.LdaModel.load(\"LDA_Models/ldamodel_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above))\n",
    "        except:\n",
    "            self.model = self.train_lda()\n",
    "            self.model.save(\"LDA_Models/ldamodel_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above))\n",
    "\n",
    "        self.corpus_feature_vectors = self.apply_lda()\n",
    "        self.final_output = self.createFinalOutput()\n",
    "\n",
    "    def createCorpus(self, filename1, filename2, stopwords):\n",
    "        with open(filename1,encoding=\"utf-8\") as f1:\n",
    "            with open(filename2, encoding=\"utf-8\") as f2:\n",
    "                all_articles = f1.readlines()\n",
    "                all_articles.extend(f2.readlines())\n",
    "                corpus = [[token for token in article.strip().split(\" \") if token not in stopwords] for article in all_articles]\n",
    "        return corpus\n",
    "\n",
    "    def createModelCorpus(self, no_below, no_above):\n",
    "        dictionary = corpora.Dictionary(self.corpus)\n",
    "        dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "        model_corpus = [dictionary.doc2bow(token) for token in self.corpus]\n",
    "        return model_corpus, dictionary\n",
    "\n",
    "    def train_lda(self):\n",
    "        '''applies LDA to tokenized articles in a txt-file where each article is\n",
    "        separated by a newsline'''\n",
    "        ldamodel = gensim.models.ldamulticore.LdaMulticore(self.model_corpus, num_topics=self.num_topics, id2word=self.dictionary, passes=10)\n",
    "        return ldamodel\n",
    "\n",
    "    def apply_lda(self):\n",
    "        corpus_feature_vectors = []\n",
    "        for article in self.model_corpus:\n",
    "            corpus_feature_vectors.append(self.model[article])\n",
    "        return corpus_feature_vectors\n",
    "\n",
    "    def createFinalOutput(self):\n",
    "        output = []\n",
    "        for vec in self.corpus_feature_vectors:\n",
    "            vec_dic = dict(vec)\n",
    "            output.append([(vec_dic[topic] if topic in vec_dic else 0) for topic in range(10)])\n",
    "        return output\n",
    "\n",
    "    def get_topics(self,num_topics=10,num_words=10,probs=False,formatted=False):\n",
    "        #return topics without probabilites\n",
    "        topics = self.model.show_topics(num_topics=num_topics,num_words=num_words,formatted=formatted)\n",
    "        #print(topics)\n",
    "        if not(probs):\n",
    "            all_topics = []\n",
    "            for topic in topics:\n",
    "                all_words = []\n",
    "                for word in topic[1]:\n",
    "                    all_words.append(word[0])\n",
    "                all_topics.append(all_words)\n",
    "                #print(all_words)\n",
    "\n",
    "            return(all_topics)\n",
    "        else:\n",
    "            return topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying LDA and having a look at the 10 topics with the 5 most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"MP\" + 0.017*\"vote\" + 0.011*\"bill\" + 0.010*\"parliament\" + 0.009*\"Labour\"'),\n",
       " (1,\n",
       "  '0.024*\"May\" + 0.012*\"Scotland\" + 0.010*\"deal\" + 0.009*\"Scottish\" + 0.009*\"referendum\"'),\n",
       " (2,\n",
       "  '0.024*\"Labour\" + 0.016*\"party\" + 0.012*\"vote\" + 0.009*\"people\" + 0.008*\"Corbyn\"'),\n",
       " (3,\n",
       "  '0.014*\"talk\" + 0.012*\"deal\" + 0.011*\"May\" + 0.010*\"European\" + 0.008*\"British\"'),\n",
       " (4,\n",
       "  '0.012*\"deal\" + 0.011*\"Ireland\" + 0.010*\"Davis\" + 0.009*\"right\" + 0.009*\"citizen\"'),\n",
       " (5,\n",
       "  '0.012*\"economy\" + 0.009*\"growth\" + 0.009*\"vote\" + 0.009*\"year\" + 0.007*\"referendum\"'),\n",
       " (6,\n",
       "  '0.011*\"May\" + 0.010*\"Johnson\" + 0.009*\"Hammond\" + 0.009*\"deal\" + 0.008*\"minister\"'),\n",
       " (7,\n",
       "  '0.013*\"trade\" + 0.011*\"Ireland\" + 0.008*\"deal\" + 0.008*\"European\" + 0.007*\"country\"'),\n",
       " (8,\n",
       "  '0.007*\"people\" + 0.007*\"European\" + 0.007*\"Trump\" + 0.007*\"vote\" + 0.006*\"year\"'),\n",
       " (9,\n",
       "  '0.012*\"financial\" + 0.011*\"business\" + 0.010*\"London\" + 0.009*\"market\" + 0.009*\"bank\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename1 = \"Corpora/filtered/filteredjust_hl_article_guardian_lemmatized.txt\"\n",
    "filename2 = \"Corpora/filtered/filteredhl_article_tele_lemmatized.txt\"\n",
    "\n",
    "stopword = stopwords.words(\"english\") + list(string.punctuation) + [\"Mr\",\"Mrs\",\"BST\",\"block-time\",\"published-time\"]\n",
    "\n",
    "lda = LDA(filename1, filename2,stopwords=stopword, num_topics=10, no_below=20, no_above=0.7)\n",
    "\n",
    "corpus_feature_vectors = lda.corpus_feature_vectors\n",
    "output = lda.final_output\n",
    "lda.get_topics(num_words=5,probs=True,formatted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means and Visualisation\n",
    "The feature vectors consist of 12 values which are the proportions in which each of the 12 topics is represented in the article. With these we can imagine every article being located somewhere in the 12-dimensional topic space. By using k-means clustering we would like to find different clusters among the articles. Being in one cluster means that a group of articles make use of a similiar topic mix. We evaluate the quality of the cluster by the \"elbow\" method using the sum of squared intra-cluster distance of the articles. We plot histograms for each cluster showing what proportions of either Guardian or Telegraph articles make up the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kmeans():\n",
    "\n",
    "    def __init__(self,feature_vector,image_dir,r_state=42):\n",
    "\n",
    "        self.feature_vector = feature_vector\n",
    "        self.r_state = r_state\n",
    "        self.dir = \"cluster_images/\"+image_dir+\"/\"\n",
    "        guardian = np.ones(998)\n",
    "        sun = np.zeros(876)\n",
    "        self.both = np.concatenate([guardian, sun])\n",
    "        self.centroids=None\n",
    "\n",
    "\n",
    "    def cluster(self,k):\n",
    "        '''\n",
    "        partitions the data into k clusters using the k_means algorithm\n",
    "        '''\n",
    "        clustering = KMeans(n_clusters=k,random_state=self.r_state)\n",
    "        labels = clustering.fit_predict(self.feature_vector)\n",
    "        self.labels = labels\n",
    "\n",
    "        self.centroids = clustering.cluster_centers_\n",
    "        return(labels,clustering.inertia_)\n",
    "\n",
    "    def visualize_data(self,fname):\n",
    "        '''\n",
    "        shows a 2-dimensional scatter plot of data with each color representing the cluster and the\n",
    "        form of the data point (triange/circle) indicates from which newspaper the article is\n",
    "        '''\n",
    "        X_reduced = PCA(n_components=2).fit_transform(self.feature_vector)\n",
    "        fig = plt.figure()\n",
    "\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=self.labels,marker=\"^\", s=self.both*10, edgecolor=\"red\", linewidth=0.3)\n",
    "        ax.scatter(X_reduced[:,0], X_reduced[:,1], c=self.labels,marker=\"o\", s=((self.both-1)*-1)*10, edgecolor=\"black\", linewidth=0.3)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        #plt.colorbar(scatter)\n",
    "\n",
    "        if not os.path.exists(self.dir):\n",
    "            os.makedirs(self.dir)\n",
    "        plt.savefig(self.dir+fname,dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_elbow(self,k_range,fname,individ_plot=False):\n",
    "        '''\n",
    "        plots the intra-cluster distance of all clustr against different number of cluster. Needs the range of k.\n",
    "        '''\n",
    "        distorsions = []\n",
    "        for k in k_range:\n",
    "            labels, score = self.cluster(k)\n",
    "            distorsions.append(score)\n",
    "            if individ_plot:\n",
    "                self.visualize_data(\"cluster_\"+str(k))\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.xlabel(\"number of clusters: k\")\n",
    "        plt.ylabel(\"Sum of squared distances of samples to their closest cluster center.\")\n",
    "        plt.plot(k_range, distorsions)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(self.dir+fname,dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_histogram2(self, fname, k):\n",
    "        '''\n",
    "        plots the histogram for the distribution of articles in each cluster\n",
    "        '''\n",
    "        grouped = [[] for x in range(k)]\n",
    "        hist = [[] for x in range(k)]\n",
    "        # the histogram of the data\n",
    "        verteilung = list(zip(self.both, self.labels))\n",
    "        verteilung = sorted(verteilung, key=lambda x: x[1])\n",
    "\n",
    "        for key, group in groupby(verteilung, lambda x: x[1]):\n",
    "            for thing in group:\n",
    "                #print(key, thing)\n",
    "                grouped[key].append(int(thing[0]))\n",
    "            hist[key].append(np.histogram(grouped[key],bins=2)[0])\n",
    "\n",
    "        guardian = []\n",
    "        telegraph = []\n",
    "        for cluster in hist:\n",
    "            guardian.append(cluster[0][0])\n",
    "            telegraph.append(cluster[0][1])\n",
    "\n",
    "        ind = np.arange(k)  # the x locations for the groups\n",
    "        width = 0.35  # the width of the bars\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        rects1 = ax.bar(ind, guardian, width, color='b')\n",
    "        rects2 = ax.bar(ind + width, telegraph, width, color='y')\n",
    "\n",
    "        # add some text for labels, title and axes ticks\n",
    "        ax.set_ylabel('Frequency in topic clusters')\n",
    "        ax.set_title('Distribution of Guardian/Telegraph articles in each cluster')\n",
    "        ax.set_xticks(ind + width / 2)\n",
    "        ax.set_xticklabels(range(k))\n",
    "\n",
    "        ax.legend((rects1[0], rects2[0]), ('The Guardian', 'Telegraph'))\n",
    "\n",
    "        print(str(self.dir + fname))\n",
    "        plt.savefig(str(self.dir + fname))\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "    # print(stopword)\n",
    "# filename1 = \"Corpora/filtered/filteredjust_hl_article_guardian_lemmatized.txt\"\n",
    "# filename2 = \"Corpora/filtered/filteredhl_article_tele_lemmatized.txt\"\n",
    "\n",
    "#     # no_below : No words which appear in less than X articles\n",
    "#     # no_above : No words which appear in more than X % of the articles\n",
    "num_topics = 10\n",
    "no_above = 0.7\n",
    "# lda = LDA(filename1, filename2, stopword,num_topics=num_topics,no_below=20, no_above=no_above)\n",
    "# corpus_feature_vectors = lda.corpus_feature_vectors\n",
    "# output = lda.final_output\n",
    "\n",
    "k_means = kmeans(output, \"num_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above).replace(\".\",\"\"))\n",
    "\n",
    "#print(k_means.centroids)\n",
    "k_means.plot_elbow(range(4,17,2), \"Elbow plot\", individ_plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after the model was applied, a kink in the elbow plot shows where a \"good\" number of clusters is reached. While we used 10 topics because this agreed with our manually annotated texts, the clustering reveals that there are actually 8 clusters which represent the same micture of topics. The Elbow plot looks as follows:\n",
    "![Pipeline](cluster_images\\num_topics=10_no_above=07\\Elbow plot.png)\n",
    "\n",
    "PCA calculates the 2 dimension with the highest explained variance and projects these dimensions of the cluster only: \n",
    "![Pipeline](cluster_images\\num_topics=10_no_above=07\\cluster_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_images/histograms/num_topics=10_no_above=07_k=8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1de8bf53240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"cluster_images/top10topics.csv\", \"a\") as top:\n",
    "    top.write(\"num_topics=\" + str(num_topics) + \"_no_above=\" + str(no_above).replace(\".\", \"\") + \",\")\n",
    "    topics = lda.get_topics(num_words=5)\n",
    "    for topic_words in topics:\n",
    "        top.write(str(topic_words) + \",\")\n",
    "        top.write(\"\\n\")\n",
    "    for k in [8]:\n",
    "        k_means = kmeans(output, \"histograms\")\n",
    "        labels,score = k_means.cluster(k)\n",
    "        k_means.plot_histogram2(\"num_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above).replace(\".\",\"\")+\"_k=\"+str(k),k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a histogram showing the distribution of Guardian and Telegraph articles in each cluster yields: \n",
    "![Pipeline](cluster_images\\histograms\\num_topics=10_no_above=07_k=8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the cluster and their centroids:\n",
    "\n",
    "![Pipeline](images\\political_stance_sample_interpretation.png)\n",
    "\n",
    "Lets investigate the clusters a bit. An exemplary interpretation for the first three cluster can be given by:\n",
    "\n",
    "If we have a closer look on Cluster 2 we can see that The Guardian published more articles than The Telegraph. The most frequent topic in Cluster 2 is Topic 2, which consisting words tell us most about the UK inner conflict with Scottish ambitions for independence. Since the SNP is against Brexit a news coverage of The Guardian is more likely.\n",
    "\n",
    "Looking at Cluster 1 we can observe a focus on the Tory party debates. Telling us, that The Telegraph is more publishing about the Conservative party.\n",
    "\n",
    "In Cluster 3 The Guardian is again the dominant newspaper, publishing more about the British-European negotiations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If articles present the same topics, the manner the topics are presented can still differ. Either a negative, positive or neutral sentiment can be expressed towards a topic. To be able to determine whether that is a the case we apply the Vader sentiment analysis system (Hutto & Gilbert, 2014) implemented in the nltk module to the headlines of the articles. We don't expect the system to work well if applied to the whole article, as many different sentiments are expressed in a text, the score would probably be balanced. Vader is a rule-based sentiment analysis system that was originally applied to user generated content. As headlines are only short texts we expected it to work well with headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt = SentimentIntensityAnalyzer()\n",
    "\n",
    "# the nltk vader sentiment analysis system is used to\n",
    "# get the polarities of the headline sentences\n",
    "# returns a list of list with the overall, negative, neutral and positive \n",
    "# value per headline\n",
    "def get_sentiment_score(sentences):\n",
    "\tscores = []\n",
    "\tfor sentence in sentences:\n",
    "\t\tscore = snt.polarity_scores(sentence)\n",
    "\t\t#score_values = [ val for key, val in score.items] not right order\n",
    "\t\tscore_values = []\n",
    "\t\tscore_values.append(str(score[\"compound\"]))\n",
    "\t\tscore_values.append(str(score[\"neg\"]))\n",
    "\t\tscore_values.append(str(score[\"neu\"]))\n",
    "\t\tscore_values.append(str(score[\"pos\"]))\n",
    "\t\tscores.append(score_values)\n",
    "\treturn scores\n",
    "\n",
    "# the scores are written to the output file \n",
    "def write_scores_to_file(out_file, scores):\n",
    "\twith open(out_file, \"w\") as o:\n",
    "\t\to.write(\"Overall score\\tNegative\\tNeutral\\tPositive\\n\")\n",
    "\t\tfor score in scores:\n",
    "\t\t\to.write(\"\\t\".join(score))\n",
    "\t\t\to.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatised_sentences = read_corpus(\"lemmatised_output_file.txt\")\n",
    "scores = get_sentiment_score(lemmatised_sentences)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of manual and automatic score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually assigned scores to 100 headlines each from The Guardian and The Telegraph. The manual and the automatic scores are compared in two manners: 1. The polarity itself is compared, it is determined whether it lays in a predefined tolerance range. 2. It is determined whether the scores have the same polarity i.e. being negative (<0), positive (>0) or neutral (=0). As the system did not perform very well - only half of the headlines had the same polarity and only very few matched within the tolerance range - only few topic words could be found in the headlines - topic words only appeared in half of the headlines at all - we did not apply sentiment analysis to further interpret the clustered topics and we also did not improve the system to an aspect-based level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only is the overall score is taken for the\n",
    "# comparison\n",
    "# it's rounded to one decimal place\n",
    "# return a list of overall scores\n",
    "def get_overall_score(scored_file):\n",
    "\toverall_scores = []\n",
    "\twith open(scored_file, 'r') as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\toverall_scores.append(round(float(line.split(\"\\t\")[0]), 1))\n",
    "\treturn overall_scores\n",
    "\n",
    "# the manual scores are one score per line\n",
    "# returns a list of manual scores\n",
    "def read_manual_sa_score(manual_sa_file):\n",
    "\tmanual_scores = []\n",
    "\twith open(manual_sa_file, 'r') as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\tmanual_scores.append(float(line.strip()))\n",
    "\treturn manual_scores\n",
    "\n",
    "# compares the automatic and manual scores\n",
    "# the ok_range is a tolerance range of how much the scores may differ\n",
    "# return the amount of right and wrong classified sentences\n",
    "# and the indices of the wrongly classified sentences\n",
    "def compare_scores(automatic, manual, ok_range):\n",
    "\tright = 0\n",
    "\twrong = 0\n",
    "\twrong_indices = []\n",
    "\tok_range = float(ok_range)\n",
    "\tfor i, (m_score, a_score) in enumerate(zip(automatic, manual)):\n",
    "\t\tif abs(m_score - a_score) <= ok_range: #and (m_score - a_score) >= 0.0:\n",
    "\t\t\tright += 1\n",
    "\t\telse:\n",
    "\t\t\twrong += 1\n",
    "\t\t\twrong_indices.append(i)\n",
    "\treturn right, wrong, wrong_indices\n",
    "\n",
    "# compares the automatic in manual scores in terms of polarity\n",
    "# if both score are below, above or equal to 0 the have the same \n",
    "# polarity\n",
    "# returns the amount of headlines with same and different polarity \n",
    "# and the indices of wrongly classified sentences\n",
    "def get_score_neg_pos_neut(automatic, manual):\n",
    "\tsame_pol = 0\n",
    "\tdiff_pol = 0\n",
    "\twrong_indices = []\n",
    "\tfor i, (m_score, a_score) in enumerate(zip(automatic, manual)):\n",
    "\t\tif (m_score > 0 and a_score > 0) or  (m_score < 0 and a_score < 0) or (m_score == 0 and a_score == 0):\n",
    "\t\t\tsame_pol += 1\n",
    "\t\telse:\n",
    "\t\t\tdiff_pol += 1\n",
    "\t\t\twrong_indices.append(i)\n",
    "\treturn same_pol, diff_pol, wrong_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract topic words from headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the headlines\n",
    "# returns a list of words per headline\n",
    "def get_headlines(article):\n",
    "\tarticles = []\n",
    "\twith open(article) as art:\n",
    "\t\tfor line in art.readlines():\n",
    "\t\t\tarticles.append(line.strip().split())\n",
    "\treturn articles\n",
    "\n",
    "# extracts the most important words per topic\n",
    "# returns a list of the words per topic\n",
    "def get_topic_words(topic_array):\n",
    "\ttopics = []\n",
    "\twith open(topic_array) as t:\n",
    "\t\tfor line in t.readlines():\n",
    "\t\t\tline = line.split(\"[\")\n",
    "\t\t\ttopic_words = []\n",
    "\t\t\tfor topic in line:\n",
    "\t\t\t\ttopic = topic.replace(\"\\'\", \"\")\n",
    "\t\t\t\ttopic = topic.replace(\"]\", \"\")\n",
    "\t\t\t\ttopic = topic.replace(\",\", \"\")\n",
    "\t\t\t\ttopic = topic.split()\n",
    "\t\t\t\ttopics.append(topic)\n",
    "\treturn(topics[1:])\n",
    "\n",
    "# reads the topic dist per article \n",
    "# returns a list of indices for the important topics  \n",
    "def get_article_topic_dist(t_dist):\n",
    "\tmost_imp = []\n",
    "\twith open(t_dist) as t:\n",
    "\t\tfor line in t.readlines():\n",
    "\t\t\tline = line.strip().split()\n",
    "\t\t\tmost_imp.append(line)\n",
    "\treturn most_imp\n",
    "\n",
    "# extracts the words that form the important topics \n",
    "# returns a list of important words per headline\n",
    "def get_imp_words(topics, topic_dist):\n",
    "\tmost_imp_words = []\n",
    "\tfor dist in topic_dist:\n",
    "\t\tmost_imp_per_art = []\n",
    "\t\tfor topic_ind in dist:\n",
    "\t\t\tmost_imp_per_art.extend(topics[int(topic_ind)])\n",
    "\t\tmost_imp_words.append(most_imp_per_art)\n",
    "\treturn most_imp_words\n",
    "\n",
    "# extract the words that are in the headline and topic words\n",
    "# returns a list of these words per article\n",
    "def get_imp_headline_words(most_imp_words, headlines):\n",
    "\twords_in_headline = []\n",
    "\tfor words, headline in zip(most_imp_words, headlines):\n",
    "\t\theadline_words = []\n",
    "\t\tfor word in words:\n",
    "\t\t\tif word in headline:\n",
    "\t\t\t\theadline_words.append(word)\n",
    "\t\twords_in_headline.append(headline_words)\n",
    "\treturn(words_in_headline)\n",
    "\n",
    "# prints the percentage of headlines in which topic words exist\n",
    "def get_percentage_imp_word_headline(most_imp_words, words_in_headline):\n",
    "\tpercentages = []\n",
    "\tfor imp_w, w_in_h in zip(most_imp_words, words_in_headline):\n",
    "\t\tif len(w_in_h) != 0:\n",
    "\t\t\tpercentage = 100/ (float(len(imp_w)) / float(len(w_in_h)))\n",
    "\t\telse:\n",
    "\t\t\tpercentage = 0\n",
    "\t\tpercentages.append(percentage)\n",
    "\tarticles_with_imp_words_in_headlines = 0\n",
    "\tten_percent_in_headline = 0\n",
    "\tfor per in percentages:\n",
    "\t\tif per > 0.0:\n",
    "\t\t\tarticles_with_imp_words_in_headlines += 1\n",
    "\t\tif per >= 10.0:\n",
    "\t\t\tten_percent_in_headline += 1\n",
    "\tprint(\"{} articles have topic words in the headlines\".format(articles_with_imp_words_in_headlines))\n",
    "\tprint(\"{} articles have more than 10% of the topics words in their headline\".format(ten_percent_in_headline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = get_topic_words(\"cluster_images/top10topicswith10wordswithoutprobs.csv\")\n",
    "topic_dist = get_article_topic_dist(\"topic_words_headline/tele_topic_dist_02.txt\")\n",
    "most_imp_words = get_imp_words(topic_words, topic_dist)\n",
    "headlines = get_headlines(\"Corpora/lemmatized/hl_tele_lemmatized.txt\")\n",
    "words_in_headline = get_imp_headline_words(most_imp_words, headlines)\n",
    "get_percentage_imp_word_headline(most_imp_words, words_in_headline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
