{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Political Stance Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A combination of topic modelling techniques and sentiment analysis applied to British newspaper articles written about the topic Brexit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](images/pipeline_ANLP_project.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tim\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "c:\\users\\tim\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import re, string, gensim, os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from tokenize_articles import read_corpus\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our data suitable for use with the topic modelling techniques & sentiment analysis, we first extract the headlines and articles, using regular expressions. This removes all metadata that is included in the downloads from LexisNexis. We then tokenise and lemmatise the articles and remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data\n",
      "\n",
      "\n",
      "                              1 of 1000 DOCUMENTS\n",
      "\n",
      "\n",
      "\n",
      "                                  The Guardian\n",
      "\n",
      "                     December 1, 2016 Thursday 8:18 PM GMT\n",
      "\n",
      "It won't be easy to stop Brexit. But here are four ways to do it;\n",
      "Chip away, every day. This is a long game but, as harsh reality bites, time will\n",
      "be on the side of the remainers\n",
      "\n",
      "BYLINE: Martin Kettle\n",
      "\n",
      "SECTION: OPINION\n",
      "\n",
      "LENGTH: 912 words\n",
      "\n",
      "\n",
      "Those of us with only a smattering of knowledge about the ancient world know one\n",
      "thing about Cato the Elder. During Rome's long wars against Hannibal, Cato ended\n",
      "every speech in the senate with the same words: \"Carthage must be destroyed.\"\n",
      "\n",
      "\" Brexit must be stopped\" is unlikely to last as long as Cato's catchphrase has\n",
      "managed to. But it focuses the mind. Those who think Brexit must be stopped are\n",
      "not the majority. But they have a case and a cause, and they are right. So how\n",
      "might stoppage be achieved?\n",
      "\n",
      "Probably not by a political movement headed by Tony Blair. The former prime\n",
      "minister is not heading back into frontline politics. But he is one of the\n",
      "biggest names to insist that Brexit is not yet irrevocable. He told the New\n",
      "Statesman last week that Brexit \" can be stopped if the British people decide\n",
      "that, having seen what it means, the pain-gain cost-benefit analysis doesn't\n",
      "stack up\". And on that he is absolutely right.\n",
      "\n",
      "When inflation rises and growth slows next year, make sure Brexit's role is\n",
      "clearly spelled out\n",
      "\n",
      "Blair carries so much baggage that it is inconceivable he either could or should\n",
      "play the leading role in any campaign. The Iraq war was wrong and it is no part\n",
      "of my argument that the past can be brushed aside. But Blair has serious things\n",
      "to say about Brexit that serious people ought to listen to. It's time his\n",
      "critics were big enough to give him a break. The 439-70 House of Commons vote\n",
      "this week against the SNP's effort to sanction Blair for the events of 2003 may\n",
      "suggest there is some space for the former Labour leader to at least be heard.\n",
      "But don't hold your breath.\n",
      "\n",
      "If Brexit is to be stopped it will require time, a change of public mood, and an\n",
      "alliance. The SNP, Greens, Liberal Democrats, significant parts of the Labour\n",
      "party and a minority of Tories would all have roles to play. People from outside\n",
      "politics are crucial too. This week's joint press conference by Nick Clegg,\n",
      "Chuka Umunna and Anna Soubry was a start. But as Clegg says, to turn the\n",
      "referendum around needs the people's consent, not a procedural trick. This will\n",
      "only happen if the public mood changes. Anti-Brexit campaigners should try to\n",
      "change it, and here's how.\n",
      "\n",
      "The first point is to be clear who they need to be talking to, rather than\n",
      "squabbling about which of them is entitled to do the talking. The name of this\n",
      "game is changing minds. So there is absolutely no point crafting a campaign that\n",
      "is aimed at fundamentalist Eurosceptics who never wanted Britain to be part of\n",
      "the EU in the first place. Nor is there any point in focusing on racists and\n",
      "xenophobes. People who don't like foreigners or people with coloured skin are\n",
      "not going to change their minds.\n",
      "\n",
      "But that leaves a lot of people who voted for Brexit, in hundreds of thousands\n",
      "of cases, on the basis of their own experience. They were sold a false\n",
      "prospectus by the leave campaign; but they need a better one, which offers them\n",
      "hope, support, material improvements in their lives, and confidence. Theresa May\n",
      "gets this, though she is doomed to promote Brexit and not to stop it. Those who\n",
      "want to stop Brexit need to learn this lesson: the aim of any campaign must be\n",
      "to persuade these voters that there is a better way of getting the things they\n",
      "want than leaving the EU. Don't berate, persuade. And get out of the bubble.\n",
      "\n",
      "The second key principle is to accept that this is a long game. Brexit won in\n",
      "June 2016. It won't be turned around quickly. Stopping Brexit is on the margins\n",
      "of political possibility right now. It could be that Brexit will be slowed by\n",
      "the supreme court's ruling, due in January. But that's just the start. Opponents\n",
      "of Brexit should settle in. Time is on their side.\n",
      "\n",
      "The negotiations with the EU will take a minimum of two years - longer if there\n",
      "is a transitional phase. The pressure to bring things to a head will be enormous\n",
      "and will grow, both in Britain and in the other 27 EU countries. British\n",
      "opponents of Brexit must be EU reformers too.\n",
      "\n",
      "Related:  Tony Blair: Brexit could be stopped if Britons change their minds\n",
      "\n",
      "The third point is to remember Cato. Chip away, every day. Every time something\n",
      "new and troubling happens, make it clear that things would be different if\n",
      "Brexit were stopped. This week's immigration figures showed a pre-referendum\n",
      "surge. Without Brexit this wouldn't have happened. Hate crimes have\n",
      "proliferated. Brexit shares the blame for that. When inflation rises and growth\n",
      "slows next year, make sure Brexit's role is spelled out. If ministers abandon\n",
      "the single market in favour of migration curbs, make Brexit's responsibility\n",
      "clear. Unless anti-Brexit campaigners have established in the public mind that\n",
      "there is a clear and viable no-Brexit alternative, they won't be in a position\n",
      "to make the most of their opportunities.\n",
      "\n",
      "The fourth point is the other side of the same coin. The leave campaign lied\n",
      "through its teeth about the benefits of Brexit. It said there would be £350m\n",
      "extra every week for the NHS. Last week the chancellor said precisely nothing\n",
      "about any extra NHS spending in the next four years. And look what is actually\n",
      "happening to the NHS. The leave campaign landed the May government with a huge\n",
      "promise that it cannot deliver. The opposition parties need to link the two at\n",
      "every opportunity.\n",
      "\n",
      "Stopping Brexit will not be easy. Recovering from a big defeat is hard. The\n",
      "campaign is more likely to fail than to succeed. The Brexiteers will fight very\n",
      "dirty. But the prize is immense - and Hannibal was defeated in a day.\n",
      "\n",
      "LOAD-DATE: December 1, 2016\n",
      "\n",
      "LANGUAGE: ENGLISH\n",
      "\n",
      "PUBLICATION-TYPE: Newspaper\n",
      "\n",
      "JOURNAL-CODE: WEBGNS\n",
      "\n",
      "\n",
      "  Copyright 2016 The Guardian, a division of Transcontinental Media Group Inc.\n",
      "                              All Rights Reserved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"sample_for_notebook.txt\") as f_raw:\n",
    "    print(\"Raw data\\n\")\n",
    "    print(f_raw.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_headlines(cleaned_article, output_file_hl):\n",
    "    '''Extracts just the headline from the raw data and saves it to a text file. \n",
    "    Used in conjunction with the function clean_corpora.'''\n",
    "    \n",
    "    with open(output_file_hl, \"a\") as output_hl:\n",
    "        hl = re.sub(r\"(BYLINE:.*|SECTION:.*|BODY: .*)$\", \"\", cleaned_article)\n",
    "        print(hl.strip(), file=output_hl)\n",
    "        \n",
    "def clean_corpora(input_file, output_file, output_file_headlines=None, guardian=False, sun=False, telegraph=False, guardian_hl=False, \n",
    "                  sun_hl=False, telegraph_hl=False):\n",
    "    '''Takes the raw LexisNexis files as input and extracts the headlines & articles and just headlines (optional), \n",
    "    writes them to a text file. Due to subtle formatting differences between the newspapers, \n",
    "    there are slight differences in the preprocessing'''\n",
    "    \n",
    "    open(output_file, 'w').close() #clears output file\n",
    "    \n",
    "    text = open(input_file).read()\n",
    "    split_text = text.strip().split(\"All Rights Reserved\")\n",
    "    \n",
    "    with open(output_file, \"a\") as output:\n",
    "        for un_article in split_text:\n",
    "            un_article1 = un_article.replace(\"\\n\", \" \")\n",
    "            article3 = un_article1.replace(\"\\\\\", \"\")\n",
    "            article2 = re.sub(r\"(BYLINE: .* [0-9]* words|SECTION: .* LETTER|SECTION: .* words)\", \"\", article3)\n",
    "            article = re.sub(r\"(LOAD-DATE:.*|LOAD-DATE)$\", \"\", article2)\n",
    "    \n",
    "            if guardian:          \n",
    "                article_g = re.sub(r\"^(.*GMT)\", \"\", article)\n",
    "                print(article_g.strip(), file=output)\n",
    "                \n",
    "                if guardian_hl:\n",
    "                    article_g1 = re.sub(r\"^(.*GMT)\", \"\", article3)\n",
    "                    extract_headlines(article_g1, output_file_headlines)\n",
    "                               \n",
    "            if sun:\n",
    "                article_s = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;)\", \"\", article)\n",
    "                article_s = re.sub(r\"([a-zA-Z.]*@the[-]*sun\\.co\\.uk.*)$\", \"\", article_s)\n",
    "                print(article_s.strip(), file=output)\n",
    "            \n",
    "                if sun_hl:\n",
    "                    article_s1 = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;)\", \"\", article3)\n",
    "                    extract_headlines(article_s1, output_file_headlines)\n",
    "                    \n",
    "            if telegraph:\n",
    "                article_t1 = re.sub(r\"^(.*National Edition|.*Edition [0-9]*;|Scotland)\", \"\", article)\n",
    "                article_t = re.sub(r\"(Copyright [0-9]+ Telegraph Media Group Limited|BYLINE: .* BODY:|[0-9]* of [0-9]* DOCUMENTS|HEADLINE:)\", \"\", article_t1)\n",
    "                print(article_t.strip(), file=output)\n",
    "                \n",
    "                if telegraph_hl:\n",
    "                    article_t1 = re.sub(r\"(HEADLINE:|Scotland)\", \"\", article_t1)\n",
    "                    extract_headlines(article_t1, output_file_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline and article\n",
      "\n",
      "It won't be easy to stop Brexit. But here are four ways to do it; Chip away, every day. This is a long game but, as harsh reality bites, time will be on the side of the remainers     Those of us with only a smattering of knowledge about the ancient world know one thing about Cato the Elder. During Rome's long wars against Hannibal, Cato ended every speech in the senate with the same words: \"Carthage must be destroyed.\"  \" Brexit must be stopped\" is unlikely to last as long as Cato's catchphrase has managed to. But it focuses the mind. Those who think Brexit must be stopped are not the majority. But they have a case and a cause, and they are right. So how might stoppage be achieved?  Probably not by a political movement headed by Tony Blair. The former prime minister is not heading back into frontline politics. But he is one of the biggest names to insist that Brexit is not yet irrevocable. He told the New Statesman last week that Brexit \" can be stopped if the British people decide that, having seen what it means, the pain-gain cost-benefit analysis doesn't stack up\". And on that he is absolutely right.  When inflation rises and growth slows next year, make sure Brexit's role is clearly spelled out  Blair carries so much baggage that it is inconceivable he either could or should play the leading role in any campaign. The Iraq war was wrong and it is no part of my argument that the past can be brushed aside. But Blair has serious things to say about Brexit that serious people ought to listen to. It's time his critics were big enough to give him a break. The 439-70 House of Commons vote this week against the SNP's effort to sanction Blair for the events of 2003 may suggest there is some space for the former Labour leader to at least be heard. But don't hold your breath.  If Brexit is to be stopped it will require time, a change of public mood, and an alliance. The SNP, Greens, Liberal Democrats, significant parts of the Labour party and a minority of Tories would all have roles to play. People from outside politics are crucial too. This week's joint press conference by Nick Clegg, Chuka Umunna and Anna Soubry was a start. But as Clegg says, to turn the referendum around needs the people's consent, not a procedural trick. This will only happen if the public mood changes. Anti-Brexit campaigners should try to change it, and here's how.  The first point is to be clear who they need to be talking to, rather than squabbling about which of them is entitled to do the talking. The name of this game is changing minds. So there is absolutely no point crafting a campaign that is aimed at fundamentalist Eurosceptics who never wanted Britain to be part of the EU in the first place. Nor is there any point in focusing on racists and xenophobes. People who don't like foreigners or people with coloured skin are not going to change their minds.  But that leaves a lot of people who voted for Brexit, in hundreds of thousands of cases, on the basis of their own experience. They were sold a false prospectus by the leave campaign; but they need a better one, which offers them hope, support, material improvements in their lives, and confidence. Theresa May gets this, though she is doomed to promote Brexit and not to stop it. Those who want to stop Brexit need to learn this lesson: the aim of any campaign must be to persuade these voters that there is a better way of getting the things they want than leaving the EU. Don't berate, persuade. And get out of the bubble.  The second key principle is to accept that this is a long game. Brexit won in June 2016. It won't be turned around quickly. Stopping Brexit is on the margins of political possibility right now. It could be that Brexit will be slowed by the supreme court's ruling, due in January. But that's just the start. Opponents of Brexit should settle in. Time is on their side.  The negotiations with the EU will take a minimum of two years - longer if there is a transitional phase. The pressure to bring things to a head will be enormous and will grow, both in Britain and in the other 27 EU countries. British opponents of Brexit must be EU reformers too.  Related:  Tony Blair: Brexit could be stopped if Britons change their minds  The third point is to remember Cato. Chip away, every day. Every time something new and troubling happens, make it clear that things would be different if Brexit were stopped. This week's immigration figures showed a pre-referendum surge. Without Brexit this wouldn't have happened. Hate crimes have proliferated. Brexit shares the blame for that. When inflation rises and growth slows next year, make sure Brexit's role is spelled out. If ministers abandon the single market in favour of migration curbs, make Brexit's responsibility clear. Unless anti-Brexit campaigners have established in the public mind that there is a clear and viable no-Brexit alternative, they won't be in a position to make the most of their opportunities.  The fourth point is the other side of the same coin. The leave campaign lied through its teeth about the benefits of Brexit. It said there would be £350m extra every week for the NHS. Last week the chancellor said precisely nothing about any extra NHS spending in the next four years. And look what is actually happening to the NHS. The leave campaign landed the May government with a huge promise that it cannot deliver. The opposition parties need to link the two at every opportunity.  Stopping Brexit will not be easy. Recovering from a big defeat is hard. The campaign is more likely to fail than to succeed. The Brexiteers will fight very dirty. But the prize is immense - and Hannibal was defeated in a day.\n",
      "\n",
      "\n",
      "\n",
      "Headline\n",
      "\n",
      "\n",
      "\n",
      "It won't be easy to stop Brexit. But here are four ways to do it; Chip away, every day. This is a long game but, as harsh reality bites, time will be on the side of the remainers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_corpora(\"sample_for_notebook.txt\", \"sample_output.txt\", \"sample_output_headlines.txt\",\n",
    "             guardian=True, guardian_hl=True)\n",
    "\n",
    "with open(\"sample_output.txt\") as f, open(\"sample_output_headlines.txt\") as f_hl:\n",
    "    print(\"\\nHeadline and article\\n\")\n",
    "    print(f.read())\n",
    "    print(\"\\nHeadline\\n\")\n",
    "    print(f_hl.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(input_file):\n",
    "\tcorpus = []\n",
    "\twith open(input_file) as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != '\\n':\n",
    "\t\t\t\tcorpus.append(line.strip())\n",
    "\treturn corpus\n",
    "\n",
    "def tokenize_corpus(articles):\n",
    "\ttokenized_articles = []\n",
    "\tfor sentence in articles:\n",
    "\t\ttokenized_articles.append(word_tokenize(sentence))\n",
    "\treturn tokenized_articles\n",
    "\n",
    "def write_to_file(tokenized_articles, output_file):\n",
    "\twith open(output_file, 'w') as o:\n",
    "\t\tfor article in tokenized_articles:\n",
    "\t\t\to.write(\" \".join(article))\n",
    "\t\t\to.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'wo', \"n't\", 'be', 'easy', 'to', 'stop', 'Brexit', '.', 'But', 'here', 'are', 'four', 'ways', 'to', 'do', 'it', ';', 'Chip', 'away', ',', 'every', 'day', '.', 'This', 'is', 'a', 'long', 'game', 'but', ',', 'as', 'harsh', 'reality', 'bites', ',', 'time', 'will', 'be', 'on', 'the', 'side', 'of', 'the', 'remainers']]\n"
     ]
    }
   ],
   "source": [
    "sample_corpus = read_corpus(\"sample_output_headlines.txt\")\n",
    "tokenized_sample = tokenize_corpus(sample_corpus)\n",
    "write_to_file(tokenized_sample, \"tokenized_output_file.txt\")\n",
    "print(tokenized_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(input_file):\n",
    "\tsentences = []\n",
    "\twith open(input_file) as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\tsentences.append(line.strip())\n",
    "\treturn sentences\n",
    "\n",
    "def annotate_articles(sentences):\n",
    "\tannotated_articles = []\n",
    "\tfor sentence in sentences:\n",
    "\t\tsentence.lower()\n",
    "\t\tp = Popen(['/Applications/Treetagger/cmd/tree-tagger-english'], stdout=PIPE, stdin=PIPE, stderr=PIPE, encoding=\"utf8\")\n",
    "\t\tout = p.communicate(input=sentence)[0]\n",
    "\t\tarticle = []\n",
    "\t\tannotated_words = out.split(\"\\n\")\n",
    "\t\tfor word_anno in annotated_words:\n",
    "\t\t\tword_anno = word_anno.split(\"\\t\")\n",
    "\t\t\tif len(word_anno) == 3:\n",
    "\t\t\t\tannotation = (word_anno[0], (word_anno[1], word_anno[2]))\n",
    "\t\t\t\tarticle.append(annotation)\n",
    "\t\tannotated_articles.append(article)\n",
    "\treturn annotated_articles\n",
    "\n",
    "def get_lemma(articles):\n",
    "\tlemma_articles = []\n",
    "\tfor article in articles:\n",
    "\t\tlemma_article = []\n",
    "\t\tfor word, anno in article:\n",
    "\t\t\tif anno[1] == \"<unknown>\":\n",
    "\t\t\t\tlemma_article.append(word)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlemma_article.append(anno[1])\n",
    "\t\tlemma_articles.append(lemma_article)\n",
    "\treturn lemma_articles\n",
    "\n",
    "def write_to_file(lemma_articles, out_file):\n",
    "\twith open(out_file, \"w\") as out:\n",
    "\t\tfor article in lemma_articles:\n",
    "\t\t\tout.write(\" \".join(article))\n",
    "\t\t\tout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Henny kannst du das ausführen? Merci!!\n",
    "lemmatized_sentences = get_sentences(\"tokenized_output_file.txt\")\n",
    "annotated_sample = annotate_articles(lemmatized_sentences)\n",
    "lemmatised_sample = get_lemma(annotated_sample)\n",
    "write_to_file(lemmatised_sample, \"lemmatised_output_file.txt\")\n",
    "print(lemmatised_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(processed_file, output_file):\n",
    "    '''Takes the lemmatized, tokenized file as input and removes punctuation, stopwords and other strangely formatted\n",
    "    words/punctuation'''\n",
    "    \n",
    "    stop_words_nltk = list(stopwords.words('english'))\n",
    "    to_delete = [\"brexit\", \"``\", \"''\", \"'s\", \"·\", \"wo\", \"n't\", \"...\", \"@card@\"]\n",
    "    \n",
    "    stop_words = stop_words_nltk + to_delete + list(string.punctuation)\n",
    "\n",
    "    with open(output_file, \"w\") as output:\n",
    "        to_be_filtered = open(processed_file).readlines()\n",
    "        corpus_list = [line.strip() for line in to_be_filtered]\n",
    "        corpus_nlist = [each_word.split() for each_word in corpus_list]\n",
    "        for sentence in range(0, len(corpus_nlist)):\n",
    "            for word in corpus_nlist[sentence]:\n",
    "                word = re.sub(r\"^(\\')\", \"\", word) #getting rid of the ' at the beginning of some words\n",
    "                if word.lower() not in stop_words:\n",
    "                    output.write(word)\n",
    "                    output.write(\" \")\n",
    "            output.write(\"\\n\") #line break after each article/hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_stop_words(\"lemmatised_output_file.txt\", \"filtered_output_file.txt\")\n",
    "with open(\"filtered_output_file.txt\") as filtered,\n",
    "    print(filtered.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an interpretation of the content of the newspaper articles, we apply topic modeling to them. \n",
    "We use a Latent Dirichlet Allocation (LDA) model from the package gensim. \n",
    "This model assumes that our articles were written using a \"generative process\" in which every word of the article is created out of a distribution of words within a topic. \n",
    "In a first step, this distribution is created by pre-defining a number of topics and then building a model over the whole corpus. In a second step, the model is applied to the articles to find the specific topic mixture which best represents the article. These topic mixtures make up our feature vectors for the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    '''\n",
    "    This class implements a Latent Dirichlet Allocation. Initializing one instance creates a\n",
    "    model and saves it to a folder. It also uses this model to create a topic distribution\n",
    "    vector for every article in the corpora\n",
    "    '''\n",
    "    def __init__(self, filename1, filename2, num_topics=10, no_below=20, no_above=0.5):\n",
    "        self.num_topics = num_topics\n",
    "        self.corpus = self.createCorpus(filename1, filename2)\n",
    "        self.model_corpus, self.dictionary = self.createModelCorpus(no_below, no_above)\n",
    "        try:\n",
    "            self.model = gensim.models.LdaModel.load(\"LDA_Models/ldamodel_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above))\n",
    "        except:\n",
    "            self.model = self.train_lda()\n",
    "            self.model.save(\"LDA_Models/ldamodel_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above))\n",
    "\n",
    "        self.corpus_feature_vectors = self.apply_lda()\n",
    "        self.final_output = self.createFinalOutput()\n",
    "\n",
    "    def createCorpus(self, filename1, filename2):\n",
    "        ''' \n",
    "        Creating lists of tokens in a list of articles for further processing\n",
    "        '''\n",
    "        with open(filename1,encoding=\"utf-8\") as f1:\n",
    "            with open(filename2, encoding=\"utf-8\") as f2:\n",
    "                all_articles = f1.readlines()\n",
    "                all_articles.extend(f2.readlines())\n",
    "                corpus = [[token for token in article.strip().split(\" \")] for article in all_articles]\n",
    "        return corpus\n",
    "\n",
    "    def createModelCorpus(self, no_below, no_above):\n",
    "        ''' \n",
    "        Creating a model corpus and a dictionary for the lda model\n",
    "        ''' \n",
    "        dictionary = corpora.Dictionary(self.corpus)\n",
    "        dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "        model_corpus = [dictionary.doc2bow(token) for token in self.corpus]\n",
    "        return model_corpus, dictionary\n",
    "\n",
    "    def train_lda(self):\n",
    "        ''' \n",
    "        Creating a LDA model from the whole corpus\n",
    "        ''' \n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(self.model_corpus, num_topics=self.num_topics, id2word=self.dictionary, passes=10)\n",
    "        return ldamodel\n",
    "\n",
    "    def apply_lda(self):\n",
    "        ''' \n",
    "        applying the LDA model to all articles to get the feature vectors\n",
    "        ''' \n",
    "        corpus_feature_vectors = []\n",
    "        for article in self.model_corpus:\n",
    "            corpus_feature_vectors.append(self.model[article])\n",
    "        return corpus_feature_vectors\n",
    "\n",
    "    def createFinalOutput(self):\n",
    "        ''' \n",
    "        arrange the feature vectors in a list\n",
    "        ''' \n",
    "        output = []\n",
    "        for vec in self.corpus_feature_vectors:\n",
    "            vec_dic = dict(vec)\n",
    "            output.append([(vec_dic[topic] if topic in vec_dic else 0) for topic in range(10)])\n",
    "        return output\n",
    "\n",
    "    def get_topics(self):\n",
    "        ''' \n",
    "        show the topics\n",
    "        ''' \n",
    "        return(self.model.show_topics(num_topics=12,num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying LDA and having a look at the 12 topics with the 5 most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"Scotland\" + 0.013*\"Scottish\" + 0.011*\"market\" + 0.010*\"trade\" + 0.010*\"business\"'),\n",
       " (1,\n",
       "  '0.019*\"Labour\" + 0.013*\"party\" + 0.010*\"vote\" + 0.009*\"people\" + 0.006*\"Corbyn\"'),\n",
       " (2,\n",
       "  '0.011*\"business\" + 0.010*\"deal\" + 0.010*\"financial\" + 0.009*\"European\" + 0.009*\"London\"'),\n",
       " (3,\n",
       "  '0.019*\"vote\" + 0.016*\"referendum\" + 0.009*\"Scotland\" + 0.009*\"MP\" + 0.008*\"Scottish\"'),\n",
       " (4,\n",
       "  '0.010*\"May\" + 0.010*\"deal\" + 0.009*\"European\" + 0.009*\"British\" + 0.008*\"Europe\"'),\n",
       " (5,\n",
       "  '0.015*\"economy\" + 0.012*\"growth\" + 0.010*\"vote\" + 0.010*\"year\" + 0.009*\"market\"'),\n",
       " (6,\n",
       "  '0.023*\"Mr\" + 0.017*\"Johnson\" + 0.010*\"May\" + 0.010*\"Hammond\" + 0.009*\"Secretary\"'),\n",
       " (7,\n",
       "  '0.015*\"Ireland\" + 0.010*\"border\" + 0.010*\"May\" + 0.009*\"right\" + 0.008*\"Northern\"'),\n",
       " (8,\n",
       "  '0.025*\"May\" + 0.014*\"Mrs\" + 0.012*\"European\" + 0.011*\"Mr\" + 0.010*\"talk\"'),\n",
       " (9,\n",
       "  '0.008*\"report\" + 0.007*\"year\" + 0.007*\"trade\" + 0.006*\"work\" + 0.006*\"civil\"'),\n",
       " (10,\n",
       "  '0.017*\"MP\" + 0.014*\"bill\" + 0.012*\"vote\" + 0.009*\"deal\" + 0.009*\"parliament\"'),\n",
       " (11,\n",
       "  '0.012*\"Davis\" + 0.012*\"deal\" + 0.010*\"BST\" + 0.009*\"May\" + 0.008*\"block-time\"')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename1 = \"Corpora/filtered/filteredjust_hl_article_guardian_lemmatized.txt\"\n",
    "filename2 = \"Corpora/filtered/filteredhl_article_tele_lemmatized.txt\"\n",
    "\n",
    "lda = LDA(filename1, filename2, num_topics=12, no_below=20, no_above=0.7)\n",
    "\n",
    "corpus_feature_vectors = lda.corpus_feature_vectors\n",
    "output = lda.final_output\n",
    "lda.get_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means and Visualisation\n",
    "After each article is assigned a feature vector consisting of 12 values specifying the portion each article deals with one of the 12 (we tested from 6 to 14 number of topics) respective topic. By that we can imagine every article being located somewhere in the 12-dimensional topic space. By using k-means clustering we would like to find different clusters among the articles. Being in one cluster means that the articles in there make use of a similiar topic mix. We evaluate the quality of the cluster by the \"elbow\" method using the sum of squared intra-cluster distance of the articles. We plot histograms for each cluster showing how much it consists of Guardian or Telegraph articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'num_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2f8f300269cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[0mno_above\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_below\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_above\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_above\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[0mcorpus_feature_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_feature_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got multiple values for argument 'num_topics'"
     ]
    }
   ],
   "source": [
    "class kmeans():\n",
    "\n",
    "    def __init__(self,feature_vector,image_dir,r_state=42):\n",
    "\n",
    "        self.feature_vector = feature_vector\n",
    "        self.r_state = r_state\n",
    "        self.dir = \"cluster_images/\"+image_dir+\"/\"\n",
    "        guardian = np.ones(998)\n",
    "        sun = np.zeros(876)\n",
    "        self.both = np.concatenate([guardian, sun])\n",
    "        self.centroids=None\n",
    "\n",
    "\n",
    "    def cluster(self,k):\n",
    "        '''\n",
    "        partitions the data into k clusters using the k_means algorithm\n",
    "        '''\n",
    "        clustering = KMeans(n_clusters=k,random_state=self.r_state)\n",
    "        labels = clustering.fit_predict(self.feature_vector)\n",
    "        self.labels = labels\n",
    "\n",
    "        self.centroids = clustering.cluster_centers_\n",
    "        return(labels,clustering.inertia_)\n",
    "\n",
    "    def visualize_data(self,fname):\n",
    "        '''\n",
    "        shows a 2-dimensional scatter plot of data with each color representing the cluster and the\n",
    "        form of the data point (triange/circle) indicates from which newspaper the article is\n",
    "        '''\n",
    "        X_reduced = PCA(n_components=2).fit_transform(self.feature_vector)\n",
    "        fig = plt.figure()\n",
    "\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=self.labels,marker=\"^\", s=self.both*10, edgecolor=\"red\", linewidth=0.3)\n",
    "        ax.scatter(X_reduced[:,0], X_reduced[:,1], c=self.labels,marker=\"o\", s=((self.both-1)*-1)*10, edgecolor=\"black\", linewidth=0.3)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        #plt.colorbar(scatter)\n",
    "\n",
    "        if not os.path.exists(self.dir):\n",
    "            os.makedirs(self.dir)\n",
    "        plt.savefig(self.dir+fname,dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_elbow(self,k_range,fname,individ_plot=False):\n",
    "        '''\n",
    "        plots the intra-cluster distance of all clustr against different number of cluster. Needs the range of k.\n",
    "        '''\n",
    "        distorsions = []\n",
    "        for k in k_range:\n",
    "            labels, score = self.cluster(k)\n",
    "            distorsions.append(score)\n",
    "            if individ_plot:\n",
    "                self.visualize_data(\"cluster_\"+str(k))\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.xlabel(\"number of clusters: k\")\n",
    "        plt.ylabel(\"Sum of squared distances of samples to their closest cluster center.\")\n",
    "        plt.plot(k_range, distorsions)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(self.dir+fname,dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_histogram2(self, fname, k):\n",
    "        '''\n",
    "        plots the histogram for the distribution of articles in each cluster\n",
    "        '''\n",
    "        grouped = [[] for x in range(k)]\n",
    "        hist = [[] for x in range(k)]\n",
    "        # the histogram of the data\n",
    "        verteilung = list(zip(self.both, self.labels))\n",
    "        verteilung = sorted(verteilung, key=lambda x: x[1])\n",
    "\n",
    "        for key, group in groupby(verteilung, lambda x: x[1]):\n",
    "            for thing in group:\n",
    "                #print(key, thing)\n",
    "                grouped[key].append(int(thing[0]))\n",
    "            hist[key].append(np.histogram(grouped[key],bins=2)[0])\n",
    "\n",
    "        guardian = []\n",
    "        telegraph = []\n",
    "        for cluster in hist:\n",
    "            guardian.append(cluster[0][0])\n",
    "            telegraph.append(cluster[0][1])\n",
    "\n",
    "        ind = np.arange(k)  # the x locations for the groups\n",
    "        width = 0.35  # the width of the bars\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        rects1 = ax.bar(ind, guardian, width, color='b')\n",
    "        rects2 = ax.bar(ind + width, telegraph, width, color='y')\n",
    "\n",
    "        # add some text for labels, title and axes ticks\n",
    "        ax.set_ylabel('Frequency in topic clusters')\n",
    "        ax.set_title('Distribution of Guardian/Telegraph articles in each cluster')\n",
    "        ax.set_xticks(ind + width / 2)\n",
    "        ax.set_xticklabels(range(k))\n",
    "\n",
    "        ax.legend((rects1[0], rects2[0]), ('The Guardian', 'Telegraph'))\n",
    "\n",
    "        print(str(self.dir + fname))\n",
    "        plt.savefig(str(self.dir + fname))\n",
    "        plt.clf()\n",
    "\n",
    "more_stopwords = [\"Mrs\", \"Mr\"]\n",
    "stopword = stopwords.words(\"english\") + list(string.punctuation) + more_stopwords\n",
    "\n",
    "    # print(stopword)\n",
    "filename1 = \"Corpora/filtered/filteredjust_hl_article_guardian_lemmatized.txt\"\n",
    "filename2 = \"Corpora/filtered/filteredhl_article_tele_lemmatized.txt\"\n",
    "\n",
    "    # no_below : No words which appear in less than X articles\n",
    "    # no_above : No words which appear in more than X % of the articles\n",
    "num_topics = 12\n",
    "no_above = 0.6\n",
    "lda = LDA(filename1, filename2, stopword, num_topics=num_topics, no_below=20, no_above=no_above)\n",
    "corpus_feature_vectors = lda.corpus_feature_vectors\n",
    "output = lda.final_output\n",
    "\n",
    "k_means = kmeans(output, \"num_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above).replace(\".\",\"\"))\n",
    "\n",
    "#print(k_means.centroids)\n",
    "k_means.plot_elbow(range(4,17,2), \"Elbow plot\", individ_plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a cluster projected onto 2 dimensions using PCA looks like that: \n",
    "<img src=\"files\\cluster_images\\num_topics=12_no_above=06\\cluster_6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cluster_images/top10topics.csv\", \"a\") as top:\n",
    "    top.write(\"num_topics=\" + str(num_topics) + \"_no_above=\" + str(no_above).replace(\".\", \"\") + \",\")\n",
    "    topics = lda.get_topics(num_words=5)\n",
    "    for topic_words in topics:\n",
    "        top.write(str(topic_words) + \",\")\n",
    "        top.write(\"\\n\")\n",
    "        for k in [6]:\n",
    "            k_means = kmeans(output, \"histograms\")\n",
    "            labels,score = k_means.cluster(k)\n",
    "            k_means.plot_histogram2(\"num_topics=\"+str(num_topics)+\"_no_above=\"+str(no_above).replace(\".\",\"\")+\"_k=\"+str(k),k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a histogram showing the distribution of Guardian and Telegraph articles in each cluster yields: \n",
    "<img src=\"files\\cluster_images\\histograms\\num_topics=12_no_above=05_k=8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the cluster and their centroids:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snt = SentimentIntensityAnalyzer()\n",
    "\n",
    "# polarity score: dict with overall, neg, neu, pos\n",
    "def get_sentiment_score(sentences):\n",
    "\tscores = []\n",
    "\tfor sentence in sentences:\n",
    "\t\tscore = snt.polarity_scores(sentence)\n",
    "\t\t#score_values = [ val for key, val in score.items] not right order\n",
    "\t\tscore_values = []\n",
    "\t\tscore_values.append(str(score[\"compound\"]))\n",
    "\t\tscore_values.append(str(score[\"neg\"]))\n",
    "\t\tscore_values.append(str(score[\"neu\"]))\n",
    "\t\tscore_values.append(str(score[\"pos\"]))\n",
    "\t\tscores.append(score_values)\n",
    "\treturn scores\n",
    "\n",
    "def write_scores_to_file(out_file, scores):\n",
    "\twith open(out_file, \"w\") as o:\n",
    "\t\to.write(\"Overall score\\tNegative\\tNeutral\\tPositive\\n\")\n",
    "\t\tfor score in scores:\n",
    "\t\t\to.write(\"\\t\".join(score))\n",
    "\t\t\to.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of manual and automatic score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_overall_score(scored_file):\n",
    "\toverall_scores = []\n",
    "\twith open(scored_file, 'r') as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\toverall_scores.append(round(float(line.split(\"\\t\")[0]), 1))\n",
    "\treturn overall_scores\n",
    "\n",
    "def read_manual_sa_score(manual_sa_file):\n",
    "\tmanual_scores = []\n",
    "\twith open(manual_sa_file, 'r') as f:\n",
    "\t\tfor line in f.readlines():\n",
    "\t\t\tif line != \"\\n\":\n",
    "\t\t\t\tmanual_scores.append(float(line.strip()))\n",
    "\treturn manual_scores\n",
    "\n",
    "def compare_scores(automatic, manual, ok_range):\n",
    "\tright = 0\n",
    "\twrong = 0\n",
    "\twrong_indices = []\n",
    "\tok_range = float(ok_range)\n",
    "\tfor i, (m_score, a_score) in enumerate(zip(automatic, manual)):\n",
    "\t\tif m_score - a_score <= ok_range:\n",
    "\t\t\tright += 1\n",
    "\t\telse:\n",
    "\t\t\twrong += 1\n",
    "\t\t\twrong_indices.append(i)\n",
    "\treturn right, wrong, wrong_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
